{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iris_classification\n",
    "<details>\n",
    "<summary>데이터 세트 로딩 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 붓꽃 데이터 세트를 로딩합니다.\n",
    "iris = load_iris()\n",
    "iris\n",
    "iris.keys()\n",
    "# iris.data 는 iris 데이터 세트에서 피처(feature)만으로 된 데이터를 배열로 가지고 있음.\n",
    "iris_data = iris.data\n",
    "# iris.target 은 붓꽃 데이터 세트에서 레이블 데이터(0,1,2)를 배열로 가지고 있음\n",
    "iris_label = iris.target\n",
    "print(f\"iris target 값 : {iris_label}\")\n",
    "print(f\"iris taget 명 : {iris.target_names}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### pandas.DataFrame\n",
    "* class pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None)\n",
    "\n",
    "</br>\n",
    "  \n",
    "<details>\n",
    "<summary>데이터 프레임 변환 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "# 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환합니다.\n",
    "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "iris_df[\"label\"] = iris.target\n",
    "iris_df.head(3)\n",
    "3. sklearn.model_selection.train_test_split\n",
    "* sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "# 트레이닝 데이터와 테스트 데이터 분리  /  (독립변수, 종속변수, 테스트 셋 사이즈, 시드)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label,\n",
    "                                                    test_size=0.2, random_state=11)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### sklearn.tree.DecisionTreeClassifier\n",
    "* classifier 는 0,1,2 등 이산분류, regressor 는 연속 판단\n",
    "* class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "<details>\n",
    "<summary>디시전 트리와 로짓분류 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "# DecisionTreeClassifier 객체 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=11)\n",
    "# 로짓 분류 적용\n",
    "dt_lr = LogisticRegression()\n",
    "# 학습 수행\n",
    "dt_clf.fit(X_train, y_train)\n",
    "# 로짓 분류 학습 수행\n",
    "dt_lr.fit(X_train, y_train)\n",
    "# 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행.\n",
    "pred = dt_clf.predict(X_test)   #predict → 테스트 셋 독립변수 집어넣어서 값 나옴\n",
    "# 로짓 분류 학습 결과 테스트\n",
    "pred_lr = dt_lr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"예측 정확도 : {accuracy_score(y_test, pred) : .4f}\") #y 테스트 값과 predict 수행한 값(0,1,2) 비교, 정확도 측정\n",
    "# 소수점 n자리까지 float 출력 f\"~ {변수 : .#f}\"\n",
    "# 로짓 회귀로 분류\n",
    "print(f\"예측 정확도 : {accuracy_score(y_test, pred_lr) : .4f}\")\n",
    "len(y_test) # 결과 값 / 테스트 셋 종속변수 갯수\n",
    "X_test[0]   # X_test의 첫번째 피쳐값\n",
    "pred    # X_test를 통한 예측 결과\n",
    "pred_lr\n",
    "y_test  # y_test의 실제 값\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## Boston 집 값 분석\n",
    "\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "bst = load_boston()\n",
    "bst_data = bst.data\n",
    "\n",
    "bst_df = pd.DataFrame(data=bst_data, columns=bst.feature_names)\n",
    "bst_df[\"price\"] = bst.target\n",
    "\n",
    "K_train, K_test, z_train, z_test = train_test_split(bst_data, bst.target,\n",
    "                                            test_size=0.2,random_state=11)\n",
    "\n",
    "dt_linR = LinearRegression()\n",
    "dt_linR.fit(K_train, z_train)\n",
    "\n",
    "pred_linR = dt_linR.predict(K_test)\n",
    "dt_linR.score(K_test, z_test)   # 학습시킨 회귀모형에 K_test 배열 집어넣은 결과와 z_test 비교 R² 도출\n",
    "r2_score(z_test, pred_linR)     # metrics 메소드의 r2_score 함수도 위와 같은 값 반환\n",
    "mse = mean_squared_error(z_test, pred_linR)\n",
    "\n",
    "rmse = np.sqrt(mse)     # scikit learn이 rmse 지원하지 않으므로, mse 구한 후 제곱근 값 구해야 함.-\n",
    "rmse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.scatter(pred_linR, z_test, color=\"black\")\n",
    "plt.plot(pred_linR, pred_linR, color=\"orange\", linewidth=3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### KFold 연습\n",
    "\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data    # 독립변수\n",
    "label = iris.target     # 종속변수\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)   # 디시젼트리 분류기 모형\n",
    "kFold = KFold(n_splits=5)   # 5개 폴드\n",
    "cv_accuracy = []            # 정확도 측정치 담을 리스트\n",
    "print(f\"붓꽃 데이터 세트 크기 : {features.shape[0]}\")   # 행 개수\n",
    "n_iter = 0      # 반복 횟수 카운터\n",
    "\n",
    "for train_index, test_index in kFold.split(features) :\n",
    "    # KFold 클래스의 split 메소드는 데이터를 받으면 값을 2개 반환\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "\n",
    "    # 학습 및 예측\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "\n",
    "    # 반복 시 마다 정확도 측정\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0] # 행 갯수\n",
    "    test_size = X_test.shape[0]\n",
    "    print(f\"\"\"#{n_iter} 교차 검증 정확도 : {accuracy}, 학습 데이터 크기 : {train_size},\n",
    "            검증데이터 크기 : {test_size}\"\"\")\n",
    "    print(f\"#{n_iter} 검증 세트 인덱스 : {test_index}\")\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "# 개별 iteration 별 정확도를 합하여 평균 정확도 계산\n",
    "print(f\"평균 검증 정확도 : {np.mean(cv_accuracy)}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### Stratified KFold 연습\n",
    "\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names)\n",
    "iris_df[\"label\"] = iris.target\n",
    "iris_df[\"label\"].value_counts() # 레이블의 갯수/비율 측정\n",
    "kfold = KFold(n_splits=3)   # 데이터셋 3개로 분할, 단순 KFold\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(iris_df) : # kfold의 각 분할 부분마다\n",
    "    n_iter += 1\n",
    "    label_train = iris_df[\"label\"].iloc[train_index]\n",
    "    label_test = iris_df[\"label\"].iloc[test_index]\n",
    "    print(f\"교차검증 : {n_iter}\")\n",
    "    print(f\"학습 레이블 데이터 분포 : \\n{label_train.value_counts()}\")\n",
    "    print(f\"검증 레이블 데이터 분포 : \\n{label_test.value_counts()}\")\n",
    "\n",
    "# 결과 보면 레이블 0,1,2가 모두 들어있지 않음.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "n_iter = 0\n",
    "\n",
    "# StratifiedKFold 의 경우, 스플릿할 데이터와, 분할 기준이 되는 array 필요\n",
    "for train_index, test_index in skf.split(iris_df, iris_df[\"label\"]) :\n",
    "    n_iter += 1\n",
    "    # print(train_index)  # 행 넘버 뽑음\n",
    "    # print(test_index)\n",
    "    label_train = iris_df[\"label\"].iloc[train_index]\n",
    "    label_test = iris_df[\"label\"].iloc[test_index]\n",
    "    print(f\"교차검증 : {n_iter}\")\n",
    "    print(f\"학습 레이블 데이터 분포 : \\n{label_train.value_counts()}\")\n",
    "    print(f\"검증 레이블 데이터 분포 : \\n{label_test.value_counts()}\")\n",
    "\n",
    "# StratifiedKFold는 0,1,2 레이블의 비율 일정하게 만들어줌.\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## Cross Validation Score\n",
    "#### sklearn.model_selection.cross_val_score\n",
    "* sklearn.model_selection.cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)\n",
    "* cv → 폴드 갯수\n",
    "* fit_params → 교차 검증으로 나온 최적 모형을 학습까지 시켜줌   \n",
    "dict\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris_data = load_iris()\n",
    "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "data = iris_data.data\n",
    "label = iris_data.target\n",
    "\n",
    "# 성능 지표는 정확도(accuracy), 교차 검증 세트는 3개\n",
    "scroes = cross_val_score(dt_clf, data, label, scoring = \"accuracy\", cv = 3)\n",
    "print(\"교차 검증별 정확도 : \", np.round(scroes, 4))\n",
    "print(\"평균 검증 정확도 : \", np.round(np.mean(scroes), 4))\n",
    "\n",
    "# KFold 에서 사용해야 하는 for문 없이 바로 결과 나옴\n",
    "#### GridSerchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한번에\n",
    "* class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 데이터를 로딩하고 학습데이터와 테스트 데이터 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,\n",
    "                                test_size = 0.2,\n",
    "                                random_state = 121)\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# parameter 들을 dictionary 형태로 설정\n",
    "parameters = {\"max_depth\" : [1,2,3], \"min_samples_split\":[2,3]}\n",
    "\n",
    "# 아래의 GridSearchCV 에서 사용\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# param_grid 의 하이퍼 파라미터들을 3개의 train, test_set_fold 로 \n",
    "# 나누어서 테스트 수행 설정.\n",
    "# refit = True 가 default 임. True 이면 가장 좋은 파라미터 설정으로 재학습 시킴.\n",
    "grid_dtree = GridSearchCV(dtree, param_grid = parameters, cv = 3, refit = True)\n",
    "\n",
    "# 붓꽃 Train 데이터로 param_grid 의 하이퍼 파라미터들을 순차적으로 학습/평가.\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "# GridSearchCV 결과 추출하여 DataFrame 으로 변환\n",
    "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "scores_df[[\"params\", \"mean_test_score\", \"rank_test_score\",\n",
    "            \"split0_test_score\", \"split1_test_score\", \"split2_test_score\"]]\n",
    "print(f\"GridSearchCV 최적 파라미터 : {grid_dtree.best_params_}\")\n",
    "print(f\"GridSearchCV 최고 정확도 : {grid_dtree.best_score_ : .4f}\")\n",
    "# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환\n",
    "estimator = grid_dtree.best_estimator_\n",
    "\n",
    "# GridSearchCV의best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨\n",
    "pred = estimator.predict(X_test)\n",
    "print(f\"테스트 데이터 세트 정확도 {accuracy_score(y_test, pred):.4f}\")\n",
    "X_test\n",
    "y_test\n",
    "estimator.predict([[7. , 3.2, 4.7, 1.4]])\n",
    "estimator.predict_proba([[7. , 3.2, 4.7, 1.4]]) # 확률로 보여줌.\n",
    "                            # 아마 가장 높은 확률 또는 90% 이상\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "# 데이터 인코딩\n",
    "* 데이터 전처리는 알고리즘 만큼 중요 (Garbage in, Garbage out)\n",
    "* 데이터는 문자열을 입력 값으로 허용 하지 않음  \n",
    "따라서 문자열을 인코딩하여 숫자로 변환 → feature vectorization 기법\n",
    "* Label Encoding과 One-Hot Encoding\n",
    "\n",
    "### sklearn.preprocessing.LabelEncoder\n",
    "* class sklearn.preprocessing.LabelEncoder\n",
    "\n",
    "<details>\n",
    "<summary>레이블 인코더 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 레이블 인코딩 이용  /  각 레이블에 숫자 순서대로 하나씩 붙여줌\n",
    "items = [\"TV\", \"냉장고\", \"전자렌지\", \"컴퓨터\", \"선풍기\", \"선풍기\", \"믹서\", \"믹서\"]\n",
    "\n",
    "# LabelEncoder 를 객체로 생성한 후, fit() 과 transform() 으로 label 인코딩 수행.\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print(\"인코딩 변환값:\", labels)\n",
    "\n",
    "# 넘버 붙여줌\n",
    "print(\"인코딩 클래스:\", encoder.classes_)\n",
    "# 넘버 순서대로 보여줌\n",
    "print(\"디코딩 원본 값:\", encoder.inverse_transform([4,5,2,0,1,1,3,3]))\n",
    "# 넘버를 원래 인덱스 값으로 재변환\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### sklearn.preprocessing.OneHotEncoder\n",
    "* class sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
    "\n",
    "<details>\n",
    "<summary>One-Hot 인코더 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# One-Hot 인코딩은 숫자값으로 나타낸 인덱스를 0,1 어레이로 나타내줌\n",
    "items = [\"TV\", \"냉장고\", \"전자렌지\", \"컴퓨터\", \"선풍기\", \"선풍기\", \"믹서\", \"믹서\"]\n",
    "\n",
    "# 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환합니다.\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)   # 요소 8개짜리 시리즈\n",
    "# 2차원 데이터로 변환\n",
    "labels = labels.reshape(-1, 1)      # .T 안됨, 8행 1열로 변환\n",
    "print(labels.shape)\n",
    "\n",
    "# 라벨 인코딩 대신, items에 직접 튜플로 번호 붙여줘도 가능\n",
    "# items = [(\"TV\", 1), (\"냉장고\", 2)] 등\n",
    "\n",
    "# One-Hot 인코딩 적용\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "print(\"One-Hot Encoding Data\\n\", oh_labels.toarray())   # array 형태로 출력\n",
    "print(\"Dimensions of One-Hot Encoding Data\\n\", oh_labels.shape)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "#### 판다스에서지원하는 원 핫 인코딩 API get_dummies()\n",
    "\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "items = [\"TV\", \"냉장고\", \"전자렌지\", \"컴퓨터\", \"선풍기\", \"선풍기\", \"믹서\", \"믹서\"]\n",
    "df = pd.DataFrame({\"item\" : items})\n",
    "pd.get_dummies(df)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### 피쳐 스케일링과 정규화\n",
    "* 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업 : feature scaling  \n",
    "→ 표준화(Standardization)와 정규화(Normalization)\n",
    "\n",
    "#### 표준화(Standardization)\n",
    "* 평균이 0이고 분산이 1인 Gaussian distribution으로 변화\n",
    "* $x'=\\frac{(x - \\mu(x))}{\\sigma(x)}$\n",
    "\n",
    "\n",
    "#### 정규화(Normaliztion)\n",
    "* 서로 다른 피쳐의 크기를 통일하기 위해 크기를 변환하는 개념\n",
    "* Min-Max 방식 : $x' = \\frac{x-min(x)}{max(x) - min(x)}$\n",
    "* 사이킷 런의 Normalizer 모듈은 선형대수에서의 정규화개념이 적용,   \n",
    "개별벡터의 크기를 맞추기 위해 변환\n",
    "* $x' = \\frac{x}{\\sqrt{x²+y²+z²}}$\n",
    "\n",
    "## 사이킷런 피쳐 스케일링 지원\n",
    "#### StandardScaler\n",
    "* 평균이 0, 분산이 1인 정규분포 형태로 변환\n",
    "#### MinMaxScaler\n",
    "* 데이터 값을 0과 1사이의 범위 값으로 변환(음수값이 있으면 -1~1)\n",
    "\n",
    "<details>\n",
    "<summary>평균 분산 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# 붓꽃 데이터 셋을 로딩하고 DataFrame으로 변환\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)\n",
    "\n",
    "print(f\"featrue들의 평균 값 : \\n{iris_df.mean()}\")\n",
    "print(f\"feature들의 분산 값 : \\n{iris_df.var()}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(iris.target, iris_df.iloc[:, 0], alpha=0.2)\n",
    "plt.scatter(iris.target, iris_df.iloc[:, 1], alpha=0.2)\n",
    "plt.scatter(iris.target, iris_df.iloc[:, 2], alpha=0.2)\n",
    "plt.scatter(iris.target, iris_df.iloc[:, 3], alpha=0.2)\n",
    "plt.show()\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "<summary>Standard Scaler 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성\n",
    "scaler = StandardScaler()\n",
    "# StandardScaler로 데이터 셋 변환. fit()과 transform() 호출.\n",
    "scaler.fit(iris_df) # 평균, 분산 계산이 fit\n",
    "iris_scaled = scaler.transform(iris_df) # 찾아낸 평균, 분산으로 변환수행\n",
    "\n",
    "# transform()시 scale 변환된 데이터 셋이 numpy ndarray로 반환되어 이를 DF로 변환.\n",
    "iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)\n",
    "print(f\"feature 평균 : \\n{iris_df_scaled.mean()}\")\n",
    "print(f\"feature 분산 : \\n{iris_df_scaled.var()}\")\n",
    "iris_df_scaled.describe()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(4,2,1)\n",
    "plt.scatter(iris.target, iris_df_scaled.iloc[:, 0])\n",
    "plt.subplot(4,2,2)\n",
    "plt.hist(iris_df_scaled.iloc[:, 0], bins=50)\n",
    "plt.subplot(4,2,3)\n",
    "plt.scatter(iris.target, iris_df_scaled.iloc[:, 1])\n",
    "plt.subplot(4,2,4)\n",
    "plt.hist(iris_df_scaled.iloc[:, 1], bins=50)\n",
    "plt.subplot(4,2,5)\n",
    "plt.scatter(iris.target, iris_df_scaled.iloc[:, 2])\n",
    "plt.subplot(4,2,6)\n",
    "plt.hist(iris_df_scaled.iloc[:, 2], bins=50)\n",
    "plt.subplot(4,2,7)\n",
    "plt.scatter(iris.target, iris_df_scaled.iloc[:, 3])\n",
    "plt.subplot(4,2,8)\n",
    "plt.hist(iris_df_scaled.iloc[:, 3], bins=50)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "X_train의 경우 fit 시키고, transform   /  X_test 는 그냥 transform만 시킴. X_train으로 얻은 평균분산을 통해 X_test로 추론하기 위해서임.\n",
    "\n",
    "## 피마 인디언 당뇨 분석 연습\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from modules import Eval\n",
    "\n",
    "origin = pd.read_csv(\".\\csv_data\\diabetes.csv\")\n",
    "df_dbt = pd.DataFrame(origin)\n",
    "\n",
    "# 피처 데이터 세트 X, 레이블 데이터 세트 y를 추출.\n",
    "# 맨 끝이 Outcome 컬럼으로 레이블 값임, 컬럼위치 -1 을 이용해 추출\n",
    "dbt_data = df_dbt.iloc[:, 0:-1]\n",
    "dbt_label = df_dbt.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(dbt_data, dbt_label,\n",
    "                                            test_size = 0.2, \n",
    "                                            random_state = 121, \n",
    "                                            stratify=dbt_label)\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"max_depth\" : [5, 6, 7], \"min_samples_split\":[2,3]}\n",
    "\n",
    "grid_dtree = GridSearchCV(dtree, param_grid = parameters, cv = 3, refit = True)\n",
    "\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "\n",
    "scores_df[[\"params\", \"mean_test_score\", \"rank_test_score\",\n",
    "            \"split0_test_score\", \"split1_test_score\", \"split2_test_score\"]]\n",
    "print(f\"GridSearchCV 최적 파라미터 : {grid_dtree.best_params_}\")\n",
    "print(f\"GridSearchCV 최고 정확도 : {grid_dtree.best_score_ : .4f}\")\n",
    "\n",
    "estimator = grid_dtree.best_estimator_\n",
    "\n",
    "pred = estimator.predict(X_test)\n",
    "print(f\"테스트 데이터 세트 정확도 {accuracy_score(y_test, pred):.4f}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# 로지스틱 회귀로 검증\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "pred = lr_clf.predict(X_test)\n",
    "pred_proba = lr_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "Eval.get_clf_eval(y_test, pred)\n",
    "Eval.precision_recall_curve_plot(y_test, pred_proba)\n",
    "\n",
    "plt.hist(df_dbt[\"Glucose\"], bins=10)\n",
    "\n",
    "# 0값을 검사할 피처명 리스트 객체 설정\n",
    "zero_features = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "\n",
    "# 전체 데이터 건수\n",
    "total_count = df_dbt[\"Glucose\"].count()\n",
    "\n",
    "# 피처별로 반복 하면서 데이터 값이 0인 데이터 건수 추출하고, 퍼센트 계산\n",
    "for feature in zero_features :\n",
    "    # 전체 데이터에서, 지정된 feature column에서 0이 들어있는 row 인덱스 번호 꺼내고, 다시 거기서 그 feature column만 불러옴.\n",
    "    zero_count = df_dbt[df_dbt[feature] == 0][feature].count()\n",
    "    print(f\"{feature} 0 건수는 {zero_count}, 비율은 {100*zero_count/total_count:.2f}%\")\n",
    "# zero_features 리스트 내부에 저장된 개별 피처들에 대해서 0값을 평균으로 대체\n",
    "df_dbt[zero_features] = df_dbt[zero_features].replace(0, df_dbt[zero_features].mean())\n",
    "```\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df_dbt.iloc[:, :-1]\n",
    "y = df_dbt.iloc[:, -1]\n",
    "\n",
    "# StandardScaler 클래스를 이용해 피처 데이터 세트에 일괄적으로 스케일링 적용\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2,\n",
    "                                                    random_state=156, stratify=y)\n",
    "\n",
    "# 로지스틱 회귀 적용\n",
    "lr_clf_2 = LogisticRegression()\n",
    "lr_clf_2.fit(X_train, y_train)\n",
    "pred_2 = lr_clf_2.predict(X_test)\n",
    "pred_proba_2 = lr_clf_2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "Eval.get_clf_eval(y_test, pred_2)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "# 모델평가\n",
    "## Confusion matrix\n",
    "\n",
    "||예측P|예측N||\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|실제P|TP|FN|Recall(Sensitivity)|\n",
    "|실제N|FP|TN|Specificity|\n",
    "||Precision|NegPredVal|Accuracy|\n",
    "\n",
    "\n",
    "### Accuracy = 예측 결과가 동일한 데이터 건수 / 전체 예측 데이터 건수\n",
    "* 정확도는 직관적으로 모델 예측 성능을 나타내는 평가 지표.  \n",
    "하지만 이진 분류의 경우 데이터의 구성에 따라 ML 모형의 성능 왜곡할 수 있음.   \n",
    "정확도 수치 하나만 가지고 성능을 평가하지 않음\n",
    "* 특히 정확도는 imbalanced 레이블 값 분포하에서는 적합하지 않은 지표\n",
    "### Recall = $\\frac{TP}{TP + FN}$ (예측과 실제가 모두 참 / 실제 참인 값)\n",
    "* 재현율은 실제 값이 P인 대상 중에 예측과 실제값이 P로 일치한 데이터의 비율\n",
    "* 병이 있을 때 있다고 판단한 비율\n",
    "### Precision = $\\frac{TP}{TP + FP}$ (예측과 실제가 모두 참 / 참으로 예측한 값)\n",
    "* 정밀도는 예측을 P로 한 대상 중에 예측과 실제 값이 P로 일치한 데이터의 비율\n",
    "### Specificity = $\\frac{TN}{FP + TN}$ (예측과 실제가 모두 거짓 / 실제 거짓인 값)\n",
    "* 병이 없을 때 없다고 판단한 비율\n",
    "### 정밀도와 재현율의 Trade Off\n",
    "- 정밀도와 재현율이 강조될 경우 Threshhold를 조정해 해당 수치 조정 가능\n",
    "- 상호 보완적인 수치이므로 Trade Off 작용\n",
    "- 모든 환자를 양성으로 판정 → FN = TN = , Recall = 100%\n",
    "- 확실한 경우만 양성, 나머지 모두 N → FP = 0, TP = 1\n",
    "\n",
    "<details>\n",
    "<summary>모형 평가 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def get_clf_eval(y_test, pred) :\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion)\n",
    "    print(f\"Accuracy : {accuracy:.4f}, Precision : {precision:.4f}, Recall : {recall:.4f}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from modules import DtPre\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "titanic_df = pd.read_csv(\"./csv_data/titanic_train.csv\")\n",
    "y_titanic_df = titanic_df[\"Survived\"]   # 레이블 데이터 셋 추출\n",
    "X_titanic_df = titanic_df.drop(\"Survived\", axis=1)  # 피쳐 데이터 셋에서 레이블셋은 삭제\n",
    "\n",
    "X_titanic_df = DtPre.transform_features(X_titanic_df) # 만들어둔 전처리 함수 적용\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df,\n",
    "                                                    test_size=0.2, random_state=11)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "lr_clf.fit(X_train, y_train)\n",
    "pred = lr_clf.predict(X_test)\n",
    "get_clf_eval(y_test, pred)\n",
    "\n",
    "# 사이킷런의 혼동행렬\n",
    "#  TN    FP  실제 N\n",
    "#  FN    TP  실제 P\n",
    "# 예측N 예측P\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "### ROC & AUC\n",
    "* TPR = $\\frac{TP}{TP + FN}$ (= Recall) | TPR은 1에 가까울 수록 좋다\n",
    "* FPR = $\\frac{FP}{FP + TN}$ (= $1-$Specificity)  |  FPR은 0에 가까울 수록 좋다\n",
    "* Decision Threshold를 높인다면 P로 분류되는 경우가 작아짐   \n",
    "만약 DT = 1 → TPR = FPR = 0     \n",
    "만약 DT = 0 → TPR = FPR = 1\n",
    "* 따라서 DT에 의해 Trade Off\n",
    "* ROC는 (FPR, TPR)쌍을 평면상에 그림    \n",
    "AUC는 ROC 아래 면적 → 1에 가까울 수록 좋은 수치     \n",
    "AUC = 1 → TPR = 1, FPR = 0\n",
    "### F1 Score\n",
    "* 정밀도와 재현율을 결합한 지표, 어느 한쪽으로 치우치지 않는 수치일 때   \n",
    "상대적으로 높은 값을 가짐 : 기하평균\n",
    "* F1 = $\\frac{2}{\\frac{1}{recall} + \\frac{1}{precision}}$ = $2\\frac{precision \\times recall}{precision + recall}$\n",
    "## 정확도의 함정\n",
    "* 더미 분류기 만들어도 데이터의 성질 또는 분포에 따라 정확도 높을 수 있음\n",
    "\n",
    "<details>\n",
    "<summary>더미 분류기 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyDummyClassifier(BaseEstimator) :\n",
    "    # fit() 메소드는 아무것도 학습하지 않음\n",
    "    def fit(self, X, y=None) :\n",
    "        pass\n",
    "    # predict() 메소드는 단순히 Sex feature가 1 이면 0, 그렇지 않으면 1로 예측함\n",
    "    def predict(self, X) :\n",
    "        pred = np.zeros((X.shape[0], 1))\n",
    "        for i in range(X.shape[0]) :\n",
    "            if X[\"Sex\"].iloc[i] == 1 :\n",
    "                pred[i] = 0\n",
    "            else :\n",
    "                pred[i] = 1\n",
    "                \n",
    "        return pred\n",
    "# 위에서 생성한 DummyClassifier에 적용\n",
    "myclf = MyDummyClassifier()\n",
    "myclf.fit(X_train, y_train)\n",
    "\n",
    "mypredictions = myclf.predict(X_test)\n",
    "print(f\"DummyClassifier의 정확도는 : {accuracy_score(y_test, mypredictions):.4f}\")\n",
    "# 아무런 학습도 시키지 않고 단순히 여자면 살았다고 판단하기만 해도 83%정확도\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "class MyFakeClassifier(BaseEstimator) :\n",
    "    # 학습 안함\n",
    "    def fit(self, X, y) :\n",
    "        pass\n",
    "\n",
    "    # 입력값으로 들어오는 X 데이터 셋의 크기만큼 0 행렬로 반환\n",
    "    def predict(self, X) :\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "# 사이킷런의 내장 데이터 셋인 load_digits()를 이용하여 MNIST 데이터 로딩\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.data)\n",
    "print(f\"### digits.data.shape : {digits.data.shape}\")\n",
    "print(digits.target)\n",
    "print(f\"###digits.target.shape : {digits.target.shape}\")\n",
    "digits.target == 7\n",
    "# digits 번호가 7번이면 True, 이를 astype(int)로 1로 변환, 7이 아니면 False, 0\n",
    "y = (digits.target == 7).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, y,\n",
    "                                                    random_state=11)\n",
    "# 불균형한 레이블 데이터 분포도 확인\n",
    "print(f\"레이블 테스트 세트 크기 : {y_test.shape}\")\n",
    "print(f\"테스트 세트 레이블의 0과 1의 분포도\")\n",
    "print(pd.Series(y_test).value_counts())\n",
    "\n",
    "# FakeClassifier 적용\n",
    "fakeclf = MyFakeClassifier()\n",
    "fakeclf.fit(X_train, y_train)\n",
    "fakepred = fakeclf.predict(X_test)\n",
    "print(f\"모든 예측을 0으로 하여도 정확도는 : {accuracy_score(y_test, fakepred):.3f}\")\n",
    "# 앞절의 예측 결과인 fakepred와 실제 결과인 y_test의 Confusion Matrix 출력\n",
    "confusion_matrix(y_test, fakepred)\n",
    "# 정확도 높지만 정밀도와 재현율 0\n",
    "print(f\"정밀도 : {precision_score(y_test, fakepred)}\")\n",
    "print(f\"재현율 : {recall_score(y_test, fakepred)}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "# 분류 결정 임곗값에 따른 Positive 예측 확률 변화\n",
    "<details>\n",
    "<summary>임곗값 연습 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "titanic_df = pd.read_csv(\"./csv_data/titanic_train.csv\")\n",
    "y_titanic_df = titanic_df[\"Survived\"]   # 레이블 데이터 셋 추출\n",
    "X_titanic_df = titanic_df.drop(\"Survived\", axis=1)  # 피쳐 데이터 셋에서 레이블셋은 삭제\n",
    "\n",
    "X_titanic_df = DtPre.transform_features(X_titanic_df) # 만들어둔 전처리 함수 적용\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df,\n",
    "                                                    test_size=0.2, random_state=11)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "lr_clf.fit(X_train, y_train)\n",
    "pred = lr_clf.predict(X_test)\n",
    "get_clf_eval(y_test, pred)\n",
    "pred_proba = lr_clf.predict_proba(X_test)\n",
    "pred = lr_clf.predict(X_test)\n",
    "print(f\"pred_proba()결과 Shape : {pred_proba.shape}\")\n",
    "print(f\"pred_proba array에서 앞 5개만 샘플로 추출 : \\n {pred_proba[:5]}\")\n",
    "\n",
    "# 예측 확률 array와 예측 결과값 array를 concatenate하여\n",
    "# 예측 확률과 결과값을 한눈에 확인\n",
    "# pred는 [1,0,0...] 형태의 array\n",
    "pred_proba_result = np.concatenate([pred_proba, pred.reshape(-1,1)], axis=1)\n",
    "# 앞이 0, 뒤가 1이 될 확률\n",
    "print(f\"두개의 class 중에서 더 큰 확률을 클래스 값으로 예측 \\n {pred_proba_result[:5]}\")\n",
    "```\n",
    "```python\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "X = [[1, -1, 2],\n",
    "     [2, 0, 0],\n",
    "     [0, 1.1, 1.2]]\n",
    "\n",
    "# threshold 기준값보다 같거나 작으면 0을, 크면 1을 반환  /  초과\n",
    "binarizer = Binarizer(threshold=1.1)\n",
    "print(binarizer.fit_transform(X))\n",
    "# Binarizer의 threshold 설정값, 분류 결정 임곗값\n",
    "custom_threshold = 0.5\n",
    "\n",
    "# predict_proba() 반환값의 두번째 컬럼, 즉 Positive 클래스 컬럼 하나만 추출하여\n",
    "# Binarizer 적용  /  Positive 일 확률이 0.5를 초과하는가?\n",
    "pred_proba_1 = pred_proba[:, 1].reshape(-1,1)\n",
    "\n",
    "binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1) # 확률검사\n",
    "custom_predict = binarizer.transform(pred_proba_1)  # 0, 1 변환\n",
    "\n",
    "get_clf_eval(y_test, custom_predict)\n",
    "# 앞과 동일한 결과\n",
    "```\n",
    "```python\n",
    "# Binarizer의 threshold 설정값을 0.4로 변경. 분류 결정 임곗값을 낮춤.\n",
    "custom_threshold = 0.4\n",
    "\n",
    "pred_proba_1 = pred_proba[:, 1].reshape(-1,1)\n",
    "\n",
    "binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1) # 확률검사\n",
    "custom_predict = binarizer.transform(pred_proba_1)  # 0, 1 변환\n",
    "\n",
    "get_clf_eval(y_test, custom_predict)\n",
    "# 재현율 높아짐 / 정밀도 낮아짐\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "<summary>임곗값 변화에 따른 스코어 변화 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "# 테스트를 수행할 모든 임곗값을 리스트 객체로 저장\n",
    "thresholds = [0.4,0.45, 0.5, 0.55, 0.6]\n",
    "\n",
    "def get_eval_by_threshold(y_test, pred_proba_c1, thresholds) :\n",
    "    # thresholds list 객체 내의 값을 차례로 iteration 하면서 Evaluation 수행\n",
    "    for custom_threshold in thresholds :\n",
    "        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1)\n",
    "        custom_predict = binarizer.transform(pred_proba_c1)\n",
    "        print(f\"임곗값 : {custom_threshold}\")\n",
    "        get_clf_eval(y_test, custom_predict)\n",
    "\n",
    "get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)\n",
    "# 임곗값 상승할 수록 재현율 하락, 정밀도 상승 trade off\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# 레이블 값이 1일때의 예측 확률을 추출\n",
    "pred_proba_class1 = lr_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# 실제값 데이터 셋과 레이블 값이 1일 때의 예측 확률을\n",
    "# precision_recall_curve 인자로 입력\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1)\n",
    "print(f\"반환된 분류 결정 임곗값 배열의 Shape : {thresholds.shape}\")\n",
    "print(f\"반환된 precision 배열의 Shape : {precisions.shape}\")\n",
    "print(f\"반환된 recalls 배열의 Shape : {recalls.shape}\")\n",
    "\n",
    "print(f\"thresholds 5 samples : {thresholds[:5]}\")\n",
    "print(f\"precisions 5 samples : {precisions[:5]}\")\n",
    "print(f\"recalls 5 samples : {recalls[:5]}\")\n",
    "\n",
    "# 반환된 임계값 배열 행이 147건이므로 샘플 10건만 추출, 임곗값을 15 steps로 추출\n",
    "thr_index = np.arange(0, thresholds.shape[0], 15)\n",
    "print(f\"샘플 추출을 위한 임곗값 배열의 index 10개 : {thr_index}\")\n",
    "print(f\"샘플용 10개의 임곗값 : {np.round(thresholds[thr_index], 2)}\")\n",
    "\n",
    "# 15 steps 단위로 추출된 임곗값에 따른 정밀도와 재현율 값\n",
    "print(f\"샘플 임곗값별 정밀도 : {np.round(precisions[thr_index],3)}\")\n",
    "print(f\"샘플 임곗값별 재현율 : {np.round(recalls[thr_index], 3)}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def precision_recall_curve_plot(y_test, pred_proba_c1) :\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n",
    "\n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 plot 수행.\n",
    "    # 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle=\"--\", label=\"precision\")\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary], label=\"recall\")\n",
    "\n",
    "    # threshold값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    # thresholds의 값을 0.10394, 0.2~ 이런식으로 배열하고 소숫점 2자리 반올림\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n",
    "\n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel(\"Threshold value\"); plt.ylabel(\"Precision and Recall value\")\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "precision_recall_curve_plot(y_test, lr_clf.predict_log_proba(X_test)[:,1])\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## 사이킷런 ROC 곡선 및 AUC 스코어\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, pred)\n",
    "print(f\"F1 scores : {f1:.4f}\")\n",
    "def get_clf_eval(y_test, pred) :\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    # F1 스코어 추가\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion)\n",
    "    # f1 score print 추가\n",
    "    print(f\"Accuracy : {accuracy:.4f}, Precision : {precision:.4f}, Recall : {recall:.4f}, F1 :{f1:.4f}\")\n",
    "thresholds = [0.4,0.45, 0.5, 0.55, 0.6]\n",
    "pred_proba = lr_clf.predict_proba(X_test)\n",
    "get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)\n",
    "# 임곗값의 변화와 재현율, 정밀도에 따라 F1 스코어 변동\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# 레이블 값이 1일때의 예측 확률을 추출\n",
    "pred_proba_class1 = lr_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "fprs, tprs, thresholds = roc_curve(y_test, pred_proba_class1)\n",
    "# 반환된 임곗값 배열에서 샘플로 데이터를 추출하되, 임곗값을 5 steps로 추출\n",
    "# thresholds[0]는 max(예측확률)+1로 임의 설정.\n",
    "# 이를 제외하기 위해 np.arange 는 1부터 시작\n",
    "thr_index = np.arange(1, thresholds.shape[0], 5)\n",
    "print(f\"샘플 추출을 위한 임곗값 배열의 Index : {thr_index}\")\n",
    "print(f\"샘플 index로 추출한 임곗값 : {np.round(thresholds[thr_index], 2)}\")\n",
    "\n",
    "# 5 steps 단위로 추출된 임곗값에 따른 FPR, TPR 값\n",
    "print(f\"샘플 임곗값별 FPR : {np.round(fprs[thr_index], 3)}\")\n",
    "print(f\"샘플 임곗값별 TPR : {np.round(tprs[thr_index], 3)}\")\n",
    "```\n",
    "```python\n",
    "def roc_curve_plot(y_test, pred_proba_c1) :\n",
    "    # 임곗값에 따른 FPR, TPR 값을 반환 받음\n",
    "    fprs, tprs, thresholds = roc_curve(y_test, pred_proba_c1)\n",
    "\n",
    "    # ROC Curve를 plot 곡선으로 그림\n",
    "    plt.plot(fprs, tprs, label=\"ROC\")\n",
    "    # 가운데 대각선 직선을 그림\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "\n",
    "    # FPR X cnrdml Scale을 0.1 단위로 변경, X,Y 축명 설정 등\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n",
    "    plt.xlabel(\"FPR(1 - Sensitivity)\"); plt.ylabel(\"TPR(Recall)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "roc_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1])\n",
    "```\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# pred = lr_clf.predict(X_test)\n",
    "# roc_score = roc_auc_score(y_test, pred)\n",
    "\n",
    "pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
    "roc_score = roc_auc_score(y_test, pred_proba)\n",
    "print(f\"ROC AUC 값 : {roc_score:.4f}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "# 타이타닉 데이터를 이용한 ML 연습\n",
    "\n",
    "### 데이터 전처리\n",
    "<details>\n",
    "<summary>전처리 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "titanic_df = pd.read_csv(\"./csv_data/titanic_train.csv\")\n",
    "titanic_df.head()\n",
    "print(\"### train 데이터 정보 ### \\n\")\n",
    "print(titanic_df.info())    #891 개 중에 null 갯수 알 수 있음\n",
    "titanic_df.isnull().sum()\n",
    "titanic_df.describe()\n",
    "titanic_df[\"Age\"].fillna(titanic_df[\"Age\"].mean(), inplace=True)\n",
    "titanic_df[\"Cabin\"].fillna(\"N\", inplace=True)\n",
    "titanic_df[\"Embarked\"].fillna(\"N\", inplace=True)    # 전처리, 데이터의 특성 숙지\n",
    "print(f\"\\n {titanic_df.info()} \\n {titanic_df.isnull().sum()}\")\n",
    "print(f\"\\n Sex 값 분포 : \\n {titanic_df['Sex'].value_counts()}\")\n",
    "print(f\"\\n Cabin 값 분포 : \\n {titanic_df['Cabin'].value_counts()}\")\n",
    "print(f\"\\n Embarked 값 분포 : \\n {titanic_df['Embarked'].value_counts()}\")\n",
    "titanic_df[\"Cabin\"] = titanic_df[\"Cabin\"].str[:1]   # 첫글자만 따서 저장\n",
    "print(titanic_df[\"Cabin\"].head())\n",
    "titanic_df[\"Cabin\"].value_counts()\n",
    "titanic_df.groupby([\"Sex\", \"Survived\"])[\"Survived\"].count()\n",
    "sns.barplot(x=\"Sex\", y=\"Survived\", data=titanic_df) # 비율로 보여줌\n",
    "sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=titanic_df)\n",
    "# hue 는 각각 분할하는 것\n",
    "# 입력 age에 따라 구분값을 반환하는 함수 설정. DataFrame 의 apply lambda 식에 사용.\n",
    "def get_category(age) :\n",
    "    cat = ''\n",
    "    if age <= -1 : cat = \"Unknown\"\n",
    "    elif age <= 5 : cat = \"Baby\"\n",
    "    elif age <= 12 : cat = \"Child\"\n",
    "    elif age <= 18 : cat = \"Teenager\"\n",
    "    elif age <= 25 : cat = \"Student\"\n",
    "    elif age <= 35 : cat = \"Young Adult\"\n",
    "    elif age <= 60 : cat = \"Adult\"\n",
    "    else : cat = \"Elderly\"\n",
    "    \n",
    "    return cat\n",
    "# 막대그래프의 크기 figure를 더 크게 설정\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# X 축의 값을 순차적으로 표시하기 위한 설정  /  sns.barplot(order=)\n",
    "group_names = [\"Unknown\", \"Baby\", \"Child\", \"Teenager\", \"Student\", \"Young Adult\", \"Adult\", \"Elderly\"]\n",
    "\n",
    "# lambda 식에 위에서 생성한 get_category() 함수를 반환값으로 지정.\n",
    "# get_category(X)는 입력값으로 \"Age\" 컬럼값을 받아서 해당하는 cat 반환\n",
    "titanic_df[\"Age_cat\"] = titanic_df[\"Age\"].apply(lambda x : get_category(x))\n",
    "sns.barplot(x=\"Age_cat\", y=\"Survived\", hue=\"Sex\", data=titanic_df, order=group_names)\n",
    "titanic_df.drop(\"Age_cat\", axis=1, inplace=True)    # 한번 하고 없앰\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 스트링으로 표현된 데이터들 넘버링  /  레이블 인코딩\n",
    "def encode_features(dataDF) :\n",
    "    features = [\"Cabin\", \"Sex\", \"Embarked\"]\n",
    "    for feature in features :\n",
    "        le = LabelEncoder()\n",
    "        le = le.fit(dataDF[feature])\n",
    "        dataDF[feature] = le.transform(dataDF[feature])\n",
    "\n",
    "    return dataDF\n",
    "\n",
    "titanic_df = encode_features(titanic_df)\n",
    "titanic_df.head()\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### 전처리 과정을 함수로 리팩토링 → 학습 데이터에 적용하여 쉽게 처리\n",
    "<details>\n",
    "<summary>전처리 함수 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "# Null 처리 함수\n",
    "def fillna(df) :\n",
    "    df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n",
    "    df[\"Cabin\"].fillna(\"N\", inplace=True)\n",
    "    df[\"Embarked\"].fillna(\"N\", inplace=True)\n",
    "    df[\"Fare\"].fillna(0, inplace=True)\n",
    "    return df\n",
    "# 머신러닝 알고리즘에 불필요한 속성 제거\n",
    "def drop_features(df) :\n",
    "    df.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)\n",
    "    return df\n",
    "# 레이블 인코딩 수행\n",
    "def format_features(df) :\n",
    "    df[\"Cabin\"] = df[\"Cabin\"].str[:1]\n",
    "    features = [\"Cabin\", \"Sex\", \"Embarked\"]\n",
    "    for feature in features :\n",
    "        le = LabelEncoder()\n",
    "        le = le.fit(df[feature])\n",
    "        df[feature] = le.transform(df[feature])\n",
    "    return df\n",
    "# 앞에서 설정한 Data Processing 함수 호출\n",
    "def transform_features(df) :\n",
    "    df = fillna(df)\n",
    "    df = drop_features(df)\n",
    "    df = format_features(df)\n",
    "    return df\n",
    "\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### 학습수행\n",
    "<details>\n",
    "<summary>학습용 데이터 전처리 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "titanic_df = pd.read_csv(\"./csv_data/titanic_train.csv\")\n",
    "y_titanic_df = titanic_df[\"Survived\"]   # 레이블 데이터 셋 추출\n",
    "X_titanic_df = titanic_df.drop(\"Survived\", axis=1)  # 피쳐 데이터 셋에서 레이블셋은 삭제\n",
    "\n",
    "X_titanic_df = transform_features(X_titanic_df) # 만들어둔 전처리 함수 적용\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df,\n",
    "                                                    test_size=0.2, random_state=121)\n",
    "# 학습 데이터 셋 분할\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "#### 디시전 트리, 랜덤 포레스트, 로지스틱 회귀 모형 이용\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 결정트리, Random Forest, 로지스틱 회귀를 위한 Classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=11)\n",
    "rf_clf = RandomForestClassifier(random_state=11)\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "# 디시전 트리 모형으로 검증\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "print(f\"DecisionTreeClassifier 정확도 : {accuracy_score(y_test, dt_pred) : .4f}\")\n",
    "\n",
    "# 랜덤 포레스트 모형으로 검증\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "print(f\"RandomForestClassifier 정확도 : {accuracy_score(y_test, rf_pred) : .4f}\")\n",
    "\n",
    "# 로지스틱 모형으로 검증\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "print(f\"LogisticRegression 정확도 : {accuracy_score(y_test, lr_pred) : .4f}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "#### KFold 모형 사용\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def exec_kfold(clf, folds=5) :\n",
    "    # 폴드 세트가 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위해 리스트 객체 생성\n",
    "    kfold = KFold(n_splits=folds)\n",
    "    scores = []\n",
    "\n",
    "    # KFold 교차 검증 수행\n",
    "    for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)) :\n",
    "        # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 형성\n",
    "        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n",
    "        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n",
    "\n",
    "        # Classifier 검증\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        scores.append(accuracy)\n",
    "        print(f\"교차검증 {iter_count} 정확도 : {accuracy : .4f}\")\n",
    "\n",
    "    # 5개 fold 에서의 평균 정확도 계산\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"평균 정확도 : {mean_score:.4f}\")\n",
    "\n",
    "# exec_kfold 호출  /  위에서 설정한 디시전 트리 모형 가져옴\n",
    "exec_kfold(dt_clf, folds=5)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "#### Cross_val_score 사용\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 위에서 설정한 디시전 트리 모형에 대해 피쳐, 레이블 값 넣어서 5회 교차검증\n",
    "scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)\n",
    "for iter_count, accuracy in enumerate(scores) :\n",
    "    print(f\"교차검증 {iter_count} 정확도 : {accuracy:.4f}\")\n",
    "\n",
    "print(f\"평균 정확도 : {np.mean(scores):.4f}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "#### GridSearchCV 모형 사용\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\"max_depth\" : [2, 3, 5, 10],      # 최대 깊이\n",
    "              \"min_samples_split\" : [2, 3, 5],  # 분할되기 위해 노드가 가져야 하는 최소 샘플 수\n",
    "              \"min_samples_leaf\" : [1, 5, 8]}   # 리프 노드가 가지고 있어야 할 최소 샘플 수\n",
    "\n",
    "# 위에서 미리 분할해 놓은 훈련 세트 / test_train_split\n",
    "grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring=\"accuracy\", cv=5)\n",
    "grid_dclf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"GridSearchCV 최적 하이퍼 파라미터 : {grid_dclf.best_params_}\")\n",
    "print(f\"GridSearchCV 최고 정확도 : {grid_dclf.best_score_:.4f}\")\n",
    "best_dclf = grid_dclf.best_estimator_\n",
    "\n",
    "# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행\n",
    "dpredictions = best_dclf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, dpredictions)\n",
    "print(f\"테스트 세트에서의 DecisionTreeClassifier 정확도 : {accuracy:.4f}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## 결정트리와 앙상블\n",
    "* 결정 트리는 매우 쉽고 유연하게 적용할 수 있는 알고리즘.   \n",
    "데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적음.     \n",
    "예측 성능 향상을 위해 복잡한 규칙 구조를 가져야 함.     \n",
    "이로 인해 과적합(overfitting)이 발생해 반대로 예측 성능이 저하될 수 있다는 단점\n",
    "\n",
    "* 그러나 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용.      \n",
    "앙상블은 매우 많은 여러개의 약한 학습기(예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트 하면서 예측 성능을 향상시킴.     \n",
    "결정트리가 좋은 약한 학습기가 됨(GBM, XGBoost, LightGBM 등)\n",
    "#### Decision Tree\n",
    "* 결정 트리 알고리즘은 학습을 통해 데이터에 있는 규칙을 자동으로 찾아내어 트리(Tree) 기반의 분류 규칙을 만듦(If-Else 기반 규칙).\n",
    "* 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우함.\n",
    "* 루트 노드 -(분할)-    \n",
    "규칙노드(규칙조건, 브랜치/ 서브트리 : 새로운 규칙 노드 기반의 서브 트리 생성) - (분할) -     \n",
    "                         리프노드(결정된 분류값)     \n",
    "                         규칙노드 - (분할) - ...\n",
    "# 정보 균일도 측정 방법\n",
    "### 정보 이득(Inforamtion Gain)\n",
    "* 정보 이득은 엔트로피라는 개념을 기반으로 함.   \n",
    "엔트로피는 주어진 데이터 집합의 혼잡도를 의미함.    \n",
    "서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮음.      \n",
    "정보 이득 지수는 1에서 엔트로피 지수를 뺀 값. 즉, 1-엔트로피 지수.      \n",
    "결정 트리는 이 정보 이득 지수로 분할 기준을 정함. 즉, 정보 이득이 높은 속성을 기준으로 분할함.\n",
    "### 지니 계수\n",
    "* 지니 계수는 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수.    \n",
    "0이 가장 평등, 1로 갈수록 불평등.   \n",
    "머신러닝에 적용될 때는 의미론 적으로 재해석돼 데이터가 다양한 값을 가질수록 평등, 특정 값으로 쏠릴 경우에는 불평등한 값.    \n",
    "즉, 다양성이 낮을 수록 균일도가 높다는 의미, 1로 갈수록 균일도가 높으므로 지니 계수가 높은 속성을 기준으로 분할하는 것.\n",
    "## Decision Tree 분할 규칙\n",
    "* 기본적으로 지니 계수를 이용해 데이터 세트를 분할.\n",
    "* 정보이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식트리에 노드를 분할.\n",
    "* 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류 결정.\n",
    "### Decision Tree의 특징\n",
    "* \"균일도\" 직관적이고 쉽다.\n",
    "* 트리의 크기를 사전에 제한하는 것이 성능 튜닝에 효과적.    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 모든 데이터를 만족하는 완벽한 규칙은 만들 수 없음.\n",
    "#### max_depth\n",
    "* 트리의 최대 깊이를 규정.\n",
    "* 디폴트는 None. None으로 설정하면 min_sample_split까지 완전 분할, overfitting 우려.\n",
    "#### max_features\n",
    "* 최적 분할을 위해 고려할 최대 피처 개수. 디폴트 None, 모든 피처 사용하여 분할.\n",
    "* int 형으로 지정하면 대상 피처의 개수, float형으로 지정하면 전체 피처 중 대상 피처의 퍼센트.\n",
    "* 'sqrt'는 전체 피처 중 √(전체 피처) 개수 만큼 선정\n",
    "* 'auto'로 지정하면 sqrt와 동일\n",
    "* 'log'는 전체 피처 중 log2(전체 피처 개수)선정\n",
    "* 'None'은 전체 피처 선정\n",
    "#### min_samples_split\n",
    "* 노드를 분할하기 위한 최소한의 샘플 데이터수로 과적합을 제어하는 데 사용.\n",
    "* 디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가.\n",
    "#### min_samples_leaf\n",
    "* 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수\n",
    "* min_samples_split와 유사하게 과적합 제어 용도. 그러나 비대칭적(imbalanced) 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요.\n",
    "#### max_leaf_nodes\n",
    "* 말단 노드(Leaf)의 최대 개수\n",
    "\n",
    "<details>\n",
    "<summary>디시전 트리 시각화 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DecisionTreeClassifier 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=121)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,\n",
    "                                                    test_size=0.2, random_state=11)\n",
    "\n",
    "# DecisionTreeCalssifier 학습.\n",
    "dt_clf.fit(X_train, y_train)\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names,\n",
    "                feature_names=iris_data.feature_names, impurity=True, filled=True)\n",
    "# impurity - 불순도를 gini로 보여줌\n",
    "```\n",
    "```python\n",
    "import graphviz\n",
    "\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz로 읽어서 code 상에서 시각화\n",
    "with open(\"tree.dot\") as f :\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n",
    "# max_depth=3 짜리 트리  /  보통 홀수 깊이 설정\n",
    "with open(\"tree2.dot\") as f :\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n",
    "dt_clf.feature_importances_\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# feature importance 추출\n",
    "print(f\"Feature Importance : {np.round(dt_clf.feature_importances_, 3)}\")\n",
    "\n",
    "# feature 별 importance 매핑\n",
    "for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_) :\n",
    "    print(f\"{name} : {value:.3f}\")\n",
    "\n",
    "# feature importance를 column별로 시각화 하기\n",
    "sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## 결정트리 과적합(Overfitting)\n",
    "<details>\n",
    "<summary>디시전 트리 과적합 시각화 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"3 Class values with 2 Features Sample data creation\")\n",
    "\n",
    "# 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의\n",
    "# classification 샘플 데이터 생성\n",
    "X_features, y_labels = make_classification(n_features=2, n_redundant=0,\n",
    "                                          n_informative=2, n_classes=3,\n",
    "                                          n_clusters_per_class=1, random_state=0)\n",
    "\n",
    "# plot 형태로 2개의 features로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨.\n",
    "plt.scatter(X_features[:,0], X_features[:,1], marker=\"o\", c=y_labels, s=25,\n",
    "            cmap=\"rainbow\", edgecolors=\"k\")\n",
    "```\n",
    "```python                                          \n",
    "import numpy as np\n",
    "\n",
    "# Classifier의 Decision Boundary를 시각화 하는 함수\n",
    "def visualize_boundary(model, X, y) :\n",
    "    # 서브플롯들의 형태와 개별 플롯\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # 학습 데이터 scatter plot으로 나타내기\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=\"rainbow\", edgecolors=\"k\", \n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    xlim_start, xlim_end = ax.get_xlim()\n",
    "    ylim_start, ylim_end = ax.get_ylim()\n",
    "\n",
    "    # 호출 파라미터로 들어온 training 데이터로 model 학습\n",
    "    model.fit(X,y)\n",
    "    # meshgrid 형태인 모든 좌표값으로 예측 수행\n",
    "    # meshgrid는 격자모양의 좌표평면\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim_start, xlim_end, num=200),\n",
    "                         np.linspace(ylim_start, ylim_end, num=200))\n",
    "    # np.c_는 2차원축을 기준으로 병합 → 평탄화 된 xx와 yy 결합\n",
    "    # ndarray.ravel는 평탄화\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # contourf()를 이용하여 class boundary를 visualization 수행\n",
    "    n_classes = len(np.unique(y))\n",
    "    # levels는 컨투어 라인의 갯수를 나타내는 array, 증가형태여야 함 또는 int\n",
    "    # X,y는 Z에 들어가는 좌표값들\n",
    "    # Z는 컨투어가 그려지는 높이 값들\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                          levels=np.arange(n_classes + 1) - 0.5,\n",
    "                          cmap=\"rainbow\", clim=(y.min(), y.max()),\n",
    "                          zorder=1)\n",
    "```\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 특정한 트리 생성 제약없는 결정트리의 Decision Boundary 시각화\n",
    "dt_clf_bdry = DecisionTreeClassifier()\n",
    "visualize_boundary(dt_clf_bdry, X_features, y_labels)\n",
    "# 특정한 트리 생성 min_sample_leaf 건 Decision Boundary 시각화\n",
    "dt_clf_bdry_sbj = DecisionTreeClassifier(min_samples_leaf=6)\n",
    "visualize_boundary(dt_clf_bdry_sbj, X_features, y_labels)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## Ensemble Learning\n",
    "* 앙상블의 유형은 일반적으로 Voting, Bagging, Boosting 으로 구분, 이외에 Stacking 등의 기법이 있음\n",
    "* 대표적인 배깅은 Random Forest 알고리즘이 있으며, 부스팅에는 에이다 부스팅, 그래디언트 부스팅, XGBoost, LightGBM 등이 있음.    \n",
    "배깅은 bootstrap + aggregating  /  데이터 샘플 복원추출 + 집계       \n",
    "정형 데이터의 분류나 회귀에서는 GBM 부스팅 계열의 앙상블이 전반적으로 높은 예측 성능을 나타냄.\n",
    "* 넓은 의미로는 서로 다른 모델을 결합한 것들을 앙상블로 지칭하기도 함\n",
    "#### 앙상블의 특징\n",
    "* 단일 모델의 약점을 다수의 모델들을 결합하여 보완\n",
    "* 뛰어난 성능을 가진 모델들로만 구성하는 것보다 성능이 떨어지더라도 서로 다른 유형의 모델을 섞는 것이 오히려 전체 성능에 도움이 될 수 있음\n",
    "* 랜덤 포레스트 및 뛰어난 부스팅 알고리즘들은 모두 결정 트리 알고리즘을 기반 알고리즘으로 적용함\n",
    "* 결정 트리의 단점인 overfitting을 수십~수천개의 많은 분류기를 결합해 보완하고 장점인 직관적인 분류 기준은 강화됨\n",
    "#### Voting Type\n",
    "* Hard Voting은 단순 다수결\n",
    "* Soft Voting은 클래스의 확률로 결과를 보정\n",
    "* 일반적으로 HV 보다는 SV이 예측 성능이 상대적으로 우수하여 주로 사용됨\n",
    "* 사이킷런은 VotingClassifier 클래스를 통해 Voting을 지원\n",
    "\n",
    "## Voting Classifier\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "data_df.head()\n",
    "# 개별 모델은 로지스틱 회귀와 KNN\n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "# 개별 모델을 Soft Voting 기반의 앙상블 모델로 구현한 분류기\n",
    "vo_clf = VotingClassifier(estimators=[(\"LR\",lr_clf), (\"KNN\", knn_clf)],\n",
    "                          voting=\"soft\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    test_size=0.2, random_state=156)\n",
    "\n",
    "# VotingClassifier 적용\n",
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "print(f\"Voting 분류기 정확도 : {accuracy_score(y_test, pred): .4f}\")\n",
    "\n",
    "# 개별 모델의 적용\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers :\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print(f\"{class_name} 정확도 : {accuracy_score(y_test, pred):.4f}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## Decision Tree Prac\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DF로 로드.\n",
    "featuer_name_df = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/features.txt\", sep=\"\\s+\",\n",
    "                              header=None, names=[\"column_index\", \"column_name\"])\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "featuer_name = featuer_name_df.iloc[:,1].values.tolist()\n",
    "print(f\"전체 피처명에서 10개만 추출 : {featuer_name[:10]}\")\n",
    "\n",
    "feature_dup_df = featuer_name_df.groupby(\"column_name\").count()\n",
    "print(feature_dup_df[feature_dup_df[\"column_index\"]>1].count())\n",
    "\n",
    "# 중복 feature명에 대해서는 뒤에 번호 붙이는 함수\n",
    "def get_new_feature_name_df(old_feature_name_df) :\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(\"column_name\").cumcount(),\n",
    "                                  columns=[\"dup_cnt\"])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=\"outer\")\n",
    "    new_feature_name_df[\"column_name\"] = new_feature_name_df[[\"column_name\", \"dup_cnt\"]].apply(lambda x : x[0]+\"_\"+str(x[1])\n",
    "                                                                                        if x[1]>0 else x[0], axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop([\"index\"], axis=1)\n",
    "    return new_feature_name_df\n",
    "def get_human_dataset() :\n",
    "\n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당\n",
    "    featuer_name_df = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/features.txt\", sep=\"\\s+\",\n",
    "                              header=None, names=[\"column_index\", \"column_name\"])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df() 를 이용, 신규 피처명 DF 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(featuer_name_df)\n",
    "\n",
    "    # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    featuer_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "\n",
    "    # 학습 피처 데이터 셋과 테스스 피처 데이터를 DF로 로딩, 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "    X_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "\n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DF로 로딩하고 컬럼명은 action으로 부여\n",
    "    # \"\\s+\" 데이터 사이 간격 공백으로 구분\n",
    "    y_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "    y_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "\n",
    "    # 로드된 학습/테스트용 DF를 모두 반환\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "print(\"## 학습 피처 데이터셋 info()\")\n",
    "print(X_train.info())\n",
    "print(y_train[\"action\"].value_counts())\n",
    "```\n",
    "```python\n",
    "X_train.isna().sum().sum()  # Null값 확인\n",
    "```\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f\"결정 트리 예측 정확도 : {accuracy:.4f}\")\n",
    "\n",
    "# DecisionTreeClassifier의 하이퍼 파라미터 추출\n",
    "print(f\"DecisionTreeClassifier 기본 하이퍼 파라미터 : \\n {dt_clf.get_params()}\")\n",
    "```\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"max_depth\" : [6,8,10,12,16,20,24]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=\"accuracy\", cv=5, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(f\"GridSearchCV 최고 평균 정확도 수치 : {grid_cv.best_score_}\")\n",
    "print(f\"GridSearchCV 최적 하이퍼 파라미터 : {grid_cv.best_params_}\")\n",
    "```\n",
    "```python\n",
    "# GridSearchCV 객체의 cv_results_ 속성을 DataFrame으로 생성\n",
    "cv_results_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "\n",
    "# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출\n",
    "cv_results_df[[\"param_max_depth\", \"mean_test_score\"]]\n",
    "\n",
    "# 최적 깊이 16\n",
    "max_depths = [6,8,10,12,16,20,24]\n",
    "# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정\n",
    "for depth in max_depths :\n",
    "    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    print(f\"max_depth = {depth} 정확도 {accuracy:.4f}\")\n",
    "\n",
    "# 최고 정확도 깊이 8\n",
    "params = {\n",
    "    \"max_depth\" : [8, 12, 16,20],\n",
    "    \"min_samples_split\" : [16, 24]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=\"accuracy\", cv=5, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(f\"GridSearchCV 최고 평균 정확도 수치 : {grid_cv.best_score_}\")\n",
    "print(f\"GridSearchCV 최적 하이퍼 파라미터 : {grid_cv.best_params_}\")\n",
    "```\n",
    "```python\n",
    "best_df_clf = grid_cv.best_estimator_\n",
    "pred1 = best_df_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred1)\n",
    "print(f\"결정 트리 예측 정확도 {accuracy:.4f}\")\n",
    "```\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "ftr_importance_values = best_df_clf.feature_importances_\n",
    "# Top 중요도로 정렬을 쉽게 하고, 시본의 막대그래프로 쉽게 표현하기 위해 Series변환\n",
    "ftr_importance = pd.Series(ftr_importance_values, index=X_train.columns)\n",
    "# 중요도값 순으로 Series를 정렬\n",
    "ftr_top20 = ftr_importance.sort_values(ascending=False)[:20]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Feature Importance Top 20\")\n",
    "sns.barplot(x=ftr_top20, y=ftr_top20.index)\n",
    "plt.show()\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "### Random Forest\n",
    "<details>\n",
    "<summary>랜덤 포레스트 연습 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "def get_new_feature_name_df(old_feature_name_df) :\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(\"column_name\").cumcount(),\n",
    "                                  columns=[\"dup_cnt\"])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=\"outer\")\n",
    "    new_feature_name_df[\"column_name\"] = new_feature_name_df[[\"column_name\", \"dup_cnt\"]].apply(lambda x : x[0]+\"_\"+str(x[1])\n",
    "                                                                                        if x[1]>0 else x[0], axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop([\"index\"], axis=1)\n",
    "    return new_feature_name_df\n",
    "```\n",
    "```python\n",
    "def get_human_dataset() :\n",
    "\n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당\n",
    "    featuer_name_df = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/features.txt\", sep=\"\\s+\",\n",
    "                              header=None, names=[\"column_index\", \"column_name\"])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df() 를 이용, 신규 피처명 DF 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(featuer_name_df)\n",
    "\n",
    "    # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    featuer_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "\n",
    "    # 학습 피처 데이터 셋과 테스스 피처 데이터를 DF로 로딩, 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "    X_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt\",\n",
    "                          sep=\"\\s+\", names=featuer_name)\n",
    "\n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DF로 로딩하고 컬럼명은 action으로 부여\n",
    "    # \"\\s+\" 데이터 사이 간격 공백으로 구분\n",
    "    y_train = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "    y_test = pd.read_csv(\"./csv_data/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt\",\n",
    "                          sep=\"\\s+\", header=None, names=[\"action\"])\n",
    "\n",
    "    # 로드된 학습/테스트용 DF를 모두 반환\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "```\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# 결정 트리에서 사용한 get_human_dataset()을 이용해 DF반환\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f\"랜덤 포레스트 정확도 : {accuracy:.4f}\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\" : [100],\n",
    "    \"max_depth\" : [6, 8, 10, 12],\n",
    "    \"min_samples_leaf\" : [8, 12, 18],\n",
    "    \"min_samples_split\" : [8, 16, 20]\n",
    "}\n",
    "# RandomForestClassifier 객체 생성 후 GridSerchCV 수행\n",
    "rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)  # pc의 모든 리소스 사용\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs=-1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"최적 하이퍼 파라미터 : \\n {grid_cv.best_params_}\")\n",
    "print(f\"최고 예측 정확도 : {grid_cv.best_score_:.4f}\")\n",
    "rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8,\n",
    "                                 min_samples_split=8, random_state=0)\n",
    "rf_clf1.fit(X_train, y_train)\n",
    "pred = rf_clf1.predict(X_test)\n",
    "print(f\"예측 정확도 : {accuracy_score(y_test, pred):.4f}\")\n",
    "```\n",
    "##### Feature Importance Top 20\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ftr_importance_values = rf_clf1.feature_importances_\n",
    "ftr_importance = pd.Series(ftr_importance_values, index=X_train.columns)\n",
    "ftr_top20 = ftr_importance.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Feature Importance Top 20\")\n",
    "sns.barplot(x=ftr_top20, y = ftr_top20.index)\n",
    "plt.show()\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "# 타이타닉 데이터를 이용한 최적 피처추출 적용 연습\n",
    "<details>\n",
    "<summary>코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from modules import DtPre\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "titanic_df = pd.read_csv(\"./csv_data/titanic_train.csv\")\n",
    "y_titanic_df = titanic_df[\"Survived\"]   # 레이블 데이터 셋 추출\n",
    "X_titanic_df = titanic_df.drop(\"Survived\", axis=1)  # 피쳐 데이터 셋에서 레이블셋은 삭제\n",
    "\n",
    "X_titanic_df = DtPre.transform_features(X_titanic_df) # 만들어둔 전처리 함수 적용\n",
    "\n",
    "titanic_test_df = pd.read_csv(\"./csv_data/titanic_test.csv\")\n",
    "y_titanic_test_df = titanic_test_df[\"Survived\"]   # 레이블 데이터 셋 추출\n",
    "X_titanic_test_df = titanic_test_df.drop(\"Survived\", axis=1)  # 피쳐 데이터 셋에서 레이블셋은 삭제\n",
    "\n",
    "X_titanic_test_df = DtPre.transform_features(X_titanic_test_df) # 만들어둔 전처리 함수 적용\n",
    "titanic_clf = DecisionTreeClassifier(random_state=121)\n",
    "\n",
    "titanic_clf.fit(X_titanic_df, y_titanic_df)\n",
    "# feature importance 추출\n",
    "print(f\"Feature Importance : {np.round(titanic_clf.feature_importances_, 3)}\")\n",
    "```\n",
    "```python\n",
    "# feature 별 importance 매핑\n",
    "features = []\n",
    "for name, value in zip(X_titanic_df.columns, titanic_clf.feature_importances_) :\n",
    "    print(f\"{name} : {value:.3f}\")\n",
    "    features.append({name : np.round(value, 3)})\n",
    "\n",
    "# feature importance를 column별로 시각화 하기\n",
    "sns.barplot(x=titanic_clf.feature_importances_, y=X_titanic_df.columns)\n",
    "titanic_lr = LogisticRegression()\n",
    "\n",
    "titanic_lr.fit(X_titanic_df, y_titanic_df)\n",
    "\n",
    "pred_all = titanic_lr.predict(X_titanic_df)\n",
    "\n",
    "print(np.round(accuracy_score(y_titanic_df, pred_all),3))\n",
    "X_train_feat = X_titanic_df[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Cabin\"]]\n",
    "X_test_feat = X_titanic_test_df[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Cabin\"]]\n",
    "titanic_lr_feat = LogisticRegression()\n",
    "\n",
    "titanic_lr_feat.fit(X_train_feat, y_titanic_df)\n",
    "\n",
    "pred_feat = titanic_lr_feat.predict(X_test_feat)\n",
    "\n",
    "print(np.round(accuracy_score(y_titanic_test_df, pred_feat),3))\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "# 여러가지 학습 알고리즘\n",
    "## Boosting\n",
    "* 부스팅 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 토앻 오류를 개선해 나가는 학습 방법\n",
    "* weak learner → 과적합 피하기 쉬움\n",
    "* 부스팅의 대표적인 구현은 AdaBoost(Adaptive boosting)와 그래디언트 부스트가 있음\n",
    "* 그래디언트 부스팅 → 기울기 기반 오류 감소 모형   \n",
    "일차 분류의 오류에 대해서 다시 분류, 반복\n",
    "* XGB → 손실함수 지정, 오버피팅, 소요시간 감소\n",
    "### GBM Hyper Parameters\n",
    "* loss : 경사 하강법에서 사용할 비용 함수 지정. 특별한 값 없으면 기본값인 'deviance'를 그대로 적용\n",
    "* learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률. 0~1 사이의 값.    \n",
    "너무 작으면 반복이 완료되어도 못 찾음, 반대로 너무 커도 오류 못 찾고 지나칠 수 있음.\n",
    "* n_estimators : weak learner의 개수. 개수 많으면 일정 수준까지는 좋아질 수 있음.   \n",
    "반면 개수가 많아지면 시간이 오래 걸림. 기본값 100\n",
    "* subsample : 데이터 샘플링 비율. 기본값 1, 전체 기반 학습.     \n",
    "0~1사이 설정 가능.\n",
    "\n",
    "<details>\n",
    "<summary>그래디언트 부스팅 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "from modules import HmDt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = HmDt.get_human_dataset()\n",
    "# GBM 수행 시간 측정을 위함. 시작 시간 설정.\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(f\"GBM 정확도 : {gb_accuracy:.4f}\")\n",
    "print(f\"GBM 수행시간 : {time.time() - start_time}\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100, 500],\n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(gb_clf, param_grid=params, cv=2, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(f\"최적 하이퍼 파라미터 : {grid_cv.best_params_}\")\n",
    "print(f\"최고 예측 정확도 : {grid_cv.best_score_:.4f}\")\n",
    "```\n",
    "```python\n",
    "# GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행\n",
    "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print(f\"GBM 정확도 : {gb_accuracy:.4f}\")\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## XGBoost\n",
    "#### class xgboost.XGBRegressor(*, objective='reg:squarederror', **kwargs)\n",
    "<details>\n",
    "<summary>XGB 파라미터 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "Parameters\n",
    "\n",
    "-   **n_estimators**  ([_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "    \n",
    "-   **max_depth**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – Maximum tree depth for base learners.\n",
    "    \n",
    "-   **learning_rate**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Boosting learning rate (xgb's \"eta\")\n",
    "    \n",
    "-   **verbosity**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "    \n",
    "-   **objective**  ([_Union_](https://docs.python.org/3.6/library/typing.html#typing.Union \"(in Python v3.6)\")_[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_,_ [_Callable_](https://docs.python.org/3.6/library/typing.html#typing.Callable \"(in Python v3.6)\")_[__[_[_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_,_ [_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_]__,_ [_Tuple_](https://docs.python.org/3.6/library/typing.html#typing.Tuple \"(in Python v3.6)\")_[_[_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_,_ [_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_]__]__,_ _NoneType__]_) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n",
    "    \n",
    "-   **booster**  (_Optional__[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_]_) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "    \n",
    "-   **tree_method**  (_Optional__[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_]_) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It's recommended to study this option from the parameters document:  [https://xgboost.readthedocs.io/en/latest/treemethod.html](https://xgboost.readthedocs.io/en/latest/treemethod.html).\n",
    "    \n",
    "-   **n_jobs**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n",
    "    \n",
    "-   **gamma**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    \n",
    "-   **min_child_weight**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "    \n",
    "-   **max_delta_step**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Maximum delta step we allow each tree's weight estimation to be.\n",
    "    \n",
    "-   **subsample**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of the training instance.\n",
    "    \n",
    "-   **colsample_bytree**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns when constructing each tree.\n",
    "    \n",
    "-   **colsample_bylevel**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns for each level.\n",
    "    \n",
    "-   **colsample_bynode**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns for each split.\n",
    "    \n",
    "-   **reg_alpha**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – L1 regularization term on weights (xgb's alpha).\n",
    "    \n",
    "-   **reg_lambda**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – L2 regularization term on weights (xgb's lambda).\n",
    "    \n",
    "-   **scale_pos_weight**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Balancing of positive and negative weights.\n",
    "    \n",
    "-   **base_score**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – The initial prediction score of all instances, global bias.\n",
    "    \n",
    "-   **random_state**  (_Optional__[__Union__[_[_numpy.random.RandomState_](https://numpy.org/doc/stable/reference/random/legacy.html#numpy.random.RandomState \"(in NumPy v1.22)\")_,_ [_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]__]_) –\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "<summary>XGB 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, \\\n",
    "                            recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from modules import Eval\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_set = load_breast_cancer()\n",
    "brst_data = data_set.data\n",
    "brst_labels = data_set.target\n",
    "\n",
    "brst_df = pd.DataFrame(data=brst_data, columns=data_set.feature_names)\n",
    "brst_df[\"target\"] = brst_labels\n",
    "\n",
    "brst_df.head()\n",
    "```\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(brst_data, brst_labels,\n",
    "                                                    test_size=0.3, random_state=121)\n",
    "\n",
    "brst_xgb_clt = xgb.XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "brst_xgb_clt.fit(X_train, y_train)\n",
    "\n",
    "brst_pred = brst_xgb_clt.predict(X_test)\n",
    "brst_pred_proba = brst_xgb_clt.predict_proba(X_test)\n",
    "\n",
    "Eval.get_clf_eval(y_test, brst_pred, brst_pred_proba)\n",
    "```\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "plot_importance(brst_xgb_clt, ax=ax)\n",
    "```\n",
    "\n",
    "##### XGBoost Early Stop\n",
    "```python\n",
    "brst_xgb_clt_es = xgb.XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "brst_xgb_clt_es.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=\"logloss\", eval_set=[(X_test, y_test)])\n",
    "\n",
    "brst_pred_es = brst_xgb_clt_es.predict(X_test)\n",
    "brst_pred_proba_es = brst_xgb_clt_es.predict_proba(X_test)\n",
    "\n",
    "Eval.get_clf_eval(y_test, brst_pred_es, brst_pred_proba_es)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## SVM\n",
    "* SVM은 분류에 사용되는 지도학습 머신러닝 모델이다.\n",
    "* SVM은 서포트 벡터(support vectors)를 사용해서 결정 경계(Decision Boundary)를 정의하고, 분류되지 않은 점을 해당 결정 경계와 비교해서 분류한다.\n",
    "* 서포트 벡터(support vectors)는 결정 경계에 가장 가까운 각 클래스의 점들이다.\n",
    "* 서포트 벡터와 결정 경계 사이의 거리를 마진(margin)이라고 한다.\n",
    "* SVM은 허용 가능한 오류 범위 내에서 가능한 최대 마진을 만들려고 한다.\n",
    "* 파라미터 C는 허용되는 오류 양을 조절한다. C 값이 클수록 오류를 덜 허용하며 이를 하드 마진(hard margin)이라 부른다. 반대로 C 값이 작을수록 오류를 더 많이 허용해서 소프트 마진(soft margin)을 만든다.\n",
    "* SVM에서는 선형으로 분리할 수 없는 점들을 분류하기 위해 커널(kernel)을 사용한다.\n",
    "* 커널(kernel)은 원래 가지고 있는 데이터를 더 높은 차원의 데이터로 변환한다. 2차원의 점으로 나타낼 수 있는 데이터를 다항식(polynomial) 커널은 3차원으로, RBF 커널은 점을 무한한 차원으로 변환한다.\n",
    "* RBF 커널에는 파라미터 감마(gamma)가 있다. 감마가 너무 크면 학습 데이터에 너무 의존해서 오버피팅이 발생할 수 있다.\n",
    "<details>\n",
    "<summary>SVM 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.5\n",
    ")\n",
    "```\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 감마값이 작으면 언더피팅 위험  /  C 값이 크면 하드마진에 의한 오버피팅 위험\n",
    "sv_clf = SVC(gamma=0.001, C=100.)\n",
    "sv_clf.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "sv_pred = sv_clf.predict(X_iris_test)\n",
    "print(y_iris_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_iris_test, sv_pred))\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## KNN (k-Nearest Neighbour)\n",
    "* 레이블 값을 미리 주는 지도학습\n",
    "* 장점 : 어떤 종류의 학습이나 준비 시간이 필요 없음\n",
    "* 단점 : 특징 공간에 있는 모든 데이터에 대한 정보가 필요함      \n",
    "가장 가까운 이웃을 찾기 위해 새로운 데이터에서 모든 기존 데이터까지의 거리를 확인해야 하기 때문     \n",
    "데이터와 클래스가 많이 있으면, 많은 메모리 공간과 계산 시간이 필요함\n",
    "<details>\n",
    "<summary>KNN 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "sepal = iris.data[:, 0:2]\n",
    "kind = iris.target\n",
    "\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.plot(sepal[kind==0][:,0], sepal[kind==0][:,1],\n",
    "        \"ro\", label=\"Setosa\")\n",
    "plt.plot(sepal[kind==1][:,0], sepal[kind==1][:,1],\n",
    "        \"bo\", label=\"Versicolor\")\n",
    "plt.plot(sepal[kind==2][:,0], sepal[kind==2][:,1],\n",
    "        \"yo\", label=\"Verginica\")\n",
    "\n",
    "plt.legend()\n",
    "X_i_knn = iris.data\n",
    "y_i_knn = iris.target\n",
    "\n",
    "X_i_knn_train, X_i_knn_test, y_i_knn_train, y_i_knn_test = train_test_split(\n",
    "    X_i_knn, y_i_knn, test_size=0.2, random_state=121\n",
    ")\n",
    "\n",
    "print(X_i_knn_train.shape)\n",
    "print(X_i_knn_test.shape)\n",
    "```\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "knn.fit(X_i_knn_train, y_i_knn_train)\n",
    "\n",
    "knn_pred = knn.predict(X_i_knn_test)\n",
    "print(accuracy_score(y_i_knn_test, knn_pred))\n",
    "```\n",
    "```python\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn2.fit(X_i_knn, y_i_knn)\n",
    "\n",
    "# 0 = setosa, 1 = versicolor, 2 = virginica\n",
    "classes = {\n",
    "    0 : \"Setosa\",\n",
    "    1 : \"Versicolor\",\n",
    "    2 : \"Virginica\"\n",
    "}\n",
    "\n",
    "# 새로운 데이터 제시\n",
    "X_new = [[3,4,5,2],\n",
    "        [5,4,2,2]]\n",
    "y_predict = knn.predict(X_new)\n",
    "\n",
    "print(classes[y_predict[0]])\n",
    "print(classes[y_predict[1]])\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "## Pipeline\n",
    "* 파이프라인을 사용하면 데이터 사전 처리 및 분류의 모든 단계를 포함하는 단일 개체를 만들 수 있다.\n",
    "\n",
    "* train과 test 데이터 손실을 피할 수 있다.\n",
    "* 교차 검증 및 기타 모델 선택 유형을 쉽게 만든다.\n",
    "* 재현성 증가\n",
    "<details>\n",
    "<summary>Pipeline 코드 펼치기/접기</summary>\n",
    "<div markdown=\"1\">\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_rand, y_rand = make_classification(n_samples=100, n_features=10, n_informative=2)\n",
    "\n",
    "X_rand_train, X_rand_test, y_rand_train, y_rand_test = train_test_split(X_rand, y_rand,\n",
    "                                                        test_size=0.3, random_state=121)\n",
    "\n",
    "X_rand_train.shape, X_rand_test.shape, y_rand_train.shape, y_rand_test.shape\n",
    "```\n",
    "```python\n",
    "# it takes a list of tuples parameter\n",
    "pipline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# use the pipeline object as you would do with a regular classifier\n",
    "pipline.fit(X_rand_train, y_rand_train)\n",
    "\n",
    "y_rand_pred = pipline.predict(X_rand_test)\n",
    "\n",
    "accuracy_score(y_rand_test, y_rand_pred)\n",
    "```\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "# Regression\n",
    "\n",
    "### sklearn.linear_model.LinearRegression\n",
    "* class sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
