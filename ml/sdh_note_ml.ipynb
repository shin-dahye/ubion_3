{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778ddc60",
   "metadata": {},
   "source": [
    "## 교차검증\n",
    "* 과적합 방지 위해 훈련 데이터 세트를 바꿔 훈련하면서 나온 평균을 정확도로 보는 방법\n",
    "\n",
    "#### sklearn.model_selection.KFold\n",
    "\n",
    ">class sklearn.model_selection.KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
    "\n",
    "* Split dataset into k consecutive folds (without shuffling by default)\n",
    "\n",
    "* parameter\n",
    "\n",
    "\t* n_split : split하는 개수 최소 2이상 (디폴트 값 5)\n",
    "\n",
    "\t* shuffle : 데이터 섞지 않는 False로 디폴트 값 설정\n",
    "\n",
    "#### sklearn.model_selection.StratifiedKFold\n",
    "\n",
    ">class sklearn.model_selection.StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)\n",
    "* kfold와 parameter 동일\n",
    "\n",
    "* 한쪽으로 치우친 데이터를 잘 처리하지 못하는 kfold 단점(일정한 간격으로 잘라서 사용) 보완 -> binary multi class ..\n",
    "\n",
    "* The folds are made by preserving the percentage of samples for each class. \n",
    "\t* target에 속성 값의 개수(비율)를 동일하게 가져감으로써 kfold 같이 데이터가 한 곳으로 몰리는 것 방지\n",
    "\t\n",
    "#### sklearn.model_selection.cross_val_score\n",
    "\n",
    "> sklearn.model_selection.cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)\n",
    "\n",
    "* kfold fit predict 계속 해주기 귀찮으니까 줄인거야 (교차검증 더 쉽게 하려고 사이킷런에서 api 제공)\n",
    "\n",
    "    * scores = cross_val_score(dt_clf, data, label, scoring='accuracy', cv=3) \n",
    "* parameter\n",
    "\t* estimator : data fit 하는데 사용할 object\n",
    "\t* scoring : 예측 성능 평가 지표 ('accuracy', 'neg_brier_score','top_k_accuracy' 등 엄청 많아)\n",
    "\t* cv : 교차 검증 폴드 수 (n_split과 같은 개념)\n",
    "* 반환 값은 scoring 파라미터로 지정된 측정값을 배열 형태로 반환\n",
    "\n",
    "#### sklearn.model_selection.GridSearchCV\n",
    "\n",
    "> class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
    "\n",
    "  \n",
    "\n",
    "* 교차검증과 하이퍼 파라미터 튜닝 둘 다 할 수 있는 API\n",
    "* 순차적으로 하이퍼 파라미터 입력시켜 최적 하이퍼 파라미터 도출할 수 있도록 도와줌\n",
    "\n",
    "* GridSearchCV :  GridSearch + cross_val_score\n",
    "\n",
    "* parameter\n",
    "\t* estimator : classifier, regressor, pipeline이 사용 될 수 있음\n",
    "\t* param_grid : **딕셔너리**로 입력해 파라미터 값 조정 ex) {'max_depth':[1,2,3],  'min_samples_split':[2,3]}\n",
    "\t* cv : 교차 검증 폴드 수 (분할되는 학습,데이터 셋 개수 지정)\n",
    "\t * refit = default는 True / True경우 최적의 하이퍼파라미터 대입해서 학습까지 쭉 시켜줌!\n",
    "* 반환 값은 scoring 파라미터로 지정된 측정값을 배열 형태로 반환\n",
    "* Attributes\n",
    "\t* best_estimator_ : refit = False 인 경우 안나와요\n",
    "\t* best_params_ : dict값만 저장되어 있음\n",
    "---\n",
    "## 데이터 전처리\n",
    "* 객체선언 -> fit() -> transform() \n",
    "*  train set에 fit() 시행 후 transform()시행 /  test set의 경우는 fit() 안하고 transform()만 ! (이미 fit되어있어)\n",
    "####  sklearn.preprocessing.LabelEncoder\n",
    "* 문자열 데이터들을 숫자로 변환해줌\n",
    "#### sklearn.preprocessing.OneHotEncoder\n",
    " > *class sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
    " *  parameter\n",
    "\t *  category : default는 auto -> 자동으로 범주 파악\n",
    "\t *  drop : 전체 범주 중 제외할 범주 지정 가능\n",
    "\t *  sparse : True일 경우 희소 행렬 반환, False일 경우 array 형태 반환\n",
    "\t *  dtype : 원하는 자료형으로 출력\n",
    "\n",
    "*  pandas에서 지원하는 원-핫 인코딩\n",
    "  pandas.get_dummies는 train 데이터의 특성을 학습하지 않기 때문에 train 데이터에만 있고 test 데이터에는 없는 카테고리를 test 데이터에서 원핫인코딩 된 칼럼으로 바꿔주지 않음.\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(items)\n",
    "pd.get_dummies(df)\n",
    "\n",
    "#### sklearn.preprocessing.StandardScaler\n",
    "> _class_ sklearn.preprocessing.StandardScaler(_*_, _copy=True_, _with_mean=True_, _with_std=True_)\n",
    "* parameter\n",
    "\t* copy: False일 경우 copy 대신 scale (내부 크기) 조정 (ot guaranteed to always work inplace)\n",
    "\t* with_mean: True일 경우 스케일링 전에 데이터를 중앙 배치\n",
    "\t* with_std: True일 경우 데이터를 단위 분산으로 조정\n",
    "\n",
    "####  sklearn.preprocessing.MinMaxScaler\n",
    ">class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)\n",
    "*  0 ~ 1 사이 값으로 데이터 값을 스케일링\n",
    "\n",
    "* X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "* X_scaled = X_std * (max - min) + min\n",
    "* parameter\n",
    "\t- feature_range : 변환된 데이터의 범위 지정, default는 (0, 1)\n",
    "\t- copy : 입력 값 array 형태일 경우 False\n",
    "\t- clip : 보류 데이터의 변환 값을 스케일링 범위로 자르고 싶다면 True 설정\n",
    "\n",
    "---\n",
    "## 머신러닝 평가\n",
    "* accuracy(정확도)\n",
    "\n",
    ": 전체 예측 건수에서 정답을 맞힌 건수의 비율\n",
    "정확도 역설 발생 가능 / 이진 분류의 경우 데이터 구성에 따라 ML 모델의 성능 왜곡할 수 있으므로 정확도 수치 하나만 가지고 성능 평가하지 않음 -> 불균형한 레이블 값 분포에서 적합한 평가 지표가 아님\n",
    "ex) 0이 9개, 1이 1개인 데이터에서 그냥 다 0으로 예측하는 모델이면 정확도가 90%나 됨 별로야!\n",
    "이 분류기는 정답을 True Negative로만 잔뜩 맞히는 셈\n",
    "\n",
    "* Recall  (재현율)\n",
    "\n",
    ": 실제로 정답이 True 인 것들 중에서 분류기가 True로 예측한 비율 -> 애초에 True가 발생하는 확률이 적을 때 사용하면 좋아\n",
    "ex) 언제나 False로 예측하는 분류기가 있다면 accuracy는 거의 99%를 넘기겠지만, True Positive를 찾을 수 없으니 recall이 0이 됨\n",
    "ex) 재현율 단점 - 언제나 True만 답하는 분류기 가정 )  accuracy는 낮지만 recall은 1이 됨. 이 역시 말이 안 되는 지표\n",
    "\n",
    "* Precision (정밀도)\n",
    "\n",
    ":  분류기가 True로 예측한 것 중 실제 True인 비율\n",
    "언제나 True만 답하는 분류기가 있다면 recall은 1로 나오겠지만, precision은 0에 가까울 것\n",
    "결국 precision을 사용할 때의 단점은 곧 recall을 사용할 때의 장점이 되기도 함.**recall과 precision은 서로 반대 개념의 지표** (trade-off) \n",
    "\n",
    "* F1 score\n",
    " : precision recall 균형값 (조화평균)\n",
    "\n",
    "* Confusion Matrix(혼동 행렬)\n",
    " [TN FP]\n",
    " [FN TP] 결과값 순서로 출력!\n",
    "\t\t\t\t\n",
    "#### sklearn.metrics.accuracy_score, precision_score , recall_score , confusion_matrix\n",
    "> sklearn.metrics.accuracy_score(_y_true_, _y_pred_, _*_, _normalize=True_, _sample_weight=None_)\n",
    "나머지 parameter들 동일 (y_test,pred)\n",
    "\n",
    "#### Precision/Recall Trade-off\n",
    "* 정밀도 또는 재현율이 특별히 강조돼야 할 경우 분류의 결정 임계값(Threshold) 조정을 통해 정밀도 또는 재현율의 수치를 높일 수 있음\n",
    "\n",
    "* 앞이 0 뒤가 1이 될 확률\n",
    " predict_proba() : 분류 결정 예측 확률을 반환 / 예측 레이블의 확률을 반환해 주는 함수\n",
    " ex) lr_clf.predict_proba() 분류기.predict_proba\n",
    "ex) 예측 확률 array 와 예측 결과값 array 를 concatenate 하여 예측 확률과 결과값을 한눈에 확인\n",
    "pred_proba_result = np.concatenate([pred_proba , pred.reshape(-1,1)],axis=1)\n",
    "\n",
    "#### sklearn.preprocessing.Binarizer\n",
    "> class sklearn.preprocessing.Binarizer(*, threshold=0.0, copy=True)\n",
    "\n",
    "* 임계값에 따라 데이터 이진화(특성 값을 0 또는 1로 설정). 이항 변수 변환\n",
    "\n",
    "* 임계값보다 큰 값은 1로 매핑되고 임계값보다 작거나 같은 값은 0으로 매핑\n",
    "기본 임계값이 0이면 양수 값만 1로 매핑됨\n",
    "* 임계값 낮출수록 recall 이 더 중요\n",
    "\n",
    "#### sklearn.metrics.plot_precision_recall_curve\n",
    "> sklearn.metrics.plot_precision_recall_curve(estimator, X, y, *, sample_weight=None, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
    "* sklearn.metrics.plot_precision_recall_curve(정답, 예측 확률 값)\n",
    "-> 정밀도, 재현율, 임계값을 리턴해 줌\n",
    "\n",
    "---\n",
    "## 머신러닝 분류 Classifier\n",
    "> 의사결정나무(DecisionTree), 로지스틱 회귀분석(Logistice Regression), 앙상블(Ensemble), SVM(Support Vector Machine), KNN(KNearedtNeighbors)\n",
    "\n",
    "### Decision Tree\n",
    "* 매우 쉽고 유연하게 적용 가능 / 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적음 \n",
    "* 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져야 하며 이로 인한 과적합이 발생해 예측 성능이 저하될 수도 있다는 단점 -> 앙상블에서는 오히려 장점이 됨 \n",
    "* 앙상블은 매우 많은 여러개의 약한 학습기(즉, 예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트 하면서 예측 성능을 향상시키는데, 결정 트리가 좋은 약한 학습기가 됨 (GBM,XGboost,LightGMBM 등)\n",
    "\n",
    "#### 정보 균일도 측정방법\n",
    "> 정보이득 (information gain) 이 큰 쪽으로 분류 1-엔트로피 (엔트로피가 작은쪽으로)\n",
    "> 지니계수 : 불평등 지수 / 이 값이 0이면 분류가 잘됐다~ (작은쪽으로 분류) \n",
    "> 이 두가지를 통해 튜닝 가능\n",
    "\n",
    "#### sklearn.tree.DecisionTreeClassifier\n",
    "> class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
    "* parameter\n",
    "\t* criterion :  gini가 default , entropy로 설정 가능\n",
    "\t*  max_depth : 트리의 최대 깊이 규정, 디폴트 None(완벽하게 클래스 결정 값 될 때까지 계속 분할 or 노드가 가지는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이 증가)\n",
    "\t* max_features : 최적 분할을 위해 고려할 최대 피처 개수 / 디폴트 None(데이터 세트 모든 피처 사용해 분할 수행) / int(대상 피처의 개수), float(전체 피처 중 대상 피처의 퍼센트)\n",
    "\t* min_samples_split : 노드 분할 위한 최소한의 샘플 데이터 수로 과적합 제어하는 데 사용, 디폴트 2 / 작게 설정할수록 분할되는 노드 많아져서 과적합 가능성 증가\n",
    "\t*  min_ssamples_leaf : 말단 노드가 되기 위한 최소한의 샘플 데이터 수 / 과적합 제어 용도, 비대칭적 데이터인 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요\n",
    "\t* max_leaf_nodes : 말단 노드의 최대 개수\n",
    "\n",
    "#### sklearn.tree.export_graphviz (시각화)\n",
    "\n",
    ">  sklearn.tree.export_graphviz(decision_tree, out_file=None, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, leaves_parallel=False, impurity=True, node_ids=False, proportion=False, rotate=False, rounded=False, special_characters=False, precision=3, fontname='helvetica')\n",
    "\n",
    "* parameter\n",
    "\t* impurity True가 디폴트/ False 하면 불순도 계수(지니계수) 안보임\n",
    "\t\n",
    "* 피처의 조건 -> 자식 노드를 만들기 위한 규칙 조건 , 이 조건이 없으면 리프 노드 / gini는 value=[]로 주어진 데이터 분포에서의 지니계수\n",
    "* samples : 현 규칙에 해당하는 데이터 건수 / value = [] 클래스 값 기반의 데이터 건수 (붓꽃세트 경우 1,2,3, 세토사, 버시칼라,버지니카 품종 각 해당하는 데이터 수) \n",
    "class : value 리스트 내에 가장 많은 건수를 가진 결정값\n",
    "\n",
    "#### feature selection 에도 사용할 수 있어 !\n",
    "* dt_clf.feature_importances_  변수별 중요도 확인\n",
    "ex) sns.barplot(x=dt_clf.feature_importances_,y=iris.feature_names) (시각화)\n",
    "\n",
    "#### 단점 : 과적합 / sklearn.datasets.make_classification\n",
    "> sklearn.datasets.make_classification(n_samples=100, n_features=20, *, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "\n",
    "* 과적합 위해서 사이킷런 패키지로 가상의 분류모형 가상의 데이터 생성\n",
    "파라미터 값을 어떻게 바꾸냐에 따라 그래프가 어떻게 변경되는지 확인하여 하이퍼 파라미터 맞춰줌\n",
    "---\n",
    "### 로지스틱 회귀 sklearn.linear_model.LogisticRegression\n",
    "> class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "\n",
    "---\n",
    "### 앙상블\n",
    "* 보팅, 배깅, 부스팅으로 구분 + 스태깅도 있음\n",
    "\n",
    "* 대표적인 배깅은 랜덤 포레스트 알고리즘이 있음. 부스팅은 에이다 부스팅, 그래디언트 부스팅, XGboost, LightGBM 등\n",
    "\n",
    "\t* 정형 데이터의 분류나 회귀에서는 GBM 부스팅 계열의 앙상블이 전반적으로 높은 예측 성능\n",
    "\t* 배깅은 병렬, 부스팅은 순차적 학습\n",
    "\n",
    "\t* 부스팅은 오답에 대해 높은 가중치를 부여하고, 정답에 낮은 가중치를 부여 \n",
    "오답을 정답으로 맞추기 위해 오답에 더 집중하게 됨\n",
    "부스팅은 배깅에 비해 에러가 적고 성능이 좋지만, 속도가 느리고 오버피팅 가능성이 있음\n",
    "\n",
    "#### 보팅 sklearn.ensemble.VotingClassifier\n",
    ">  class sklearn.ensemble.VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
    "-   하드 보팅 : 최종 아웃풋 결과 중 각 모델들이 가장많이 선택한 아웃풋을 최종 아웃풋으로 설정\n",
    "-   소프트 보팅 : 최종 아웃풋 결과의 확률값을 기반으로 평균을 내어, 이중 가장 확률값이 높은 아웃풋을 최종 아웃풋으로 설정\n",
    "- 단순히 여러 알고리즘을 마구잡이로 결합한다고 항상 성능이 향상되는 것은 아님에 유의\n",
    "* parameter\n",
    "\t- estimator: 사용할 분류 모델 방법 (str, estimator) 형태로 지정\n",
    "\t- voting: hard voting default / 일반적으로 soft voting 많이 사용\n",
    "\t- weights:  None default -> 균일한 가중치가 사용됨\n",
    "\t- flatten_transform: soft voting일 때만 사용 의미가 있음. True일 경우 (n_samples n_classifiers*n_classes) 행렬 반환. False일 경우 (n_classifiers, n_samples, n_classes) 행렬 반환\n",
    "\t- verbose: 작업 수행 시작 출력\n",
    "* ex) vo_clf =  VotingClassifier(estimators=[('LR',lr_clf),('KNN',knn_clf)],voting='soft')\n",
    "\n",
    "#### 배깅 sklearn.ensemble.BaggingClassifier\n",
    "\n",
    "> class sklearn.ensemble.BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
    "* 대표적 알고리즘 : 랜덤 포레스트 ( 다재 다능 알고리즘, 앙상블 알고리즘 중 비교적 빠른 수행 속도, 다양한 영역에서 높은 예측 성능)\n",
    "\n",
    "#### Random Forest sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "* class sklearn.ensemble.RandomForestClassifier(n_estimators=100(결정트리 개수 지정), *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto'(기본값이 sqrt(전체피처개수)), max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "* 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정\n",
    "\n",
    "* 개별 분류기의 기반 알고리즘은 결정트리 but 개별 트리가 학습하는 데이터 셋은 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 셋 -> 여러개의 데이터 셋을 중첩되게 분리하는 것 : 부트스트래핑 분할 방식\n",
    "\n",
    "* 원본 데이터의 건수가 10개인 학습 데이터 셋에 랜덤 포레스트를 3개의 결정 트리 기반으로 학습하려고 n_estimators = 3 으로 하이퍼 파라미터 부여할 수 ㅇ\n",
    "* parameter\n",
    "\t*  n_estimators : 모델에서 사용할 트리 갯수(학습시 생성할 트리 갯수)\n",
    "\t-   criterion : 분할 품질을 측정하는 기능 (default : gini)\n",
    "\t-   max_depth : 트리의 최대 깊이\n",
    "\t-   min_samples_split : 내부 노드를 분할하는데 필요한 최소 샘플 수 (default : 2)\n",
    "\t-   min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1)\t\n",
    "\t-  \t min_weight_fraction_leaf : min_sample_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율\n",
    "\t-   max_features : 각 노드에서 분할에 사용할 특징의 최대 수\n",
    "\t-   max_leaf_nodes : 리프 노드의 최대수\n",
    "\t-   min_impurity_decrease : 최소 불순도\n",
    "\t-   min_impurity_split : 나무 성장을 멈추기 위한 임계치\n",
    "\t-   bootstrap : 부트스트랩(중복허용 샘플링) 사용 여부\n",
    "\t-   oob_score : 일반화 정확도를 줄이기 위해 밖의 샘플 사용 여부\n",
    "\t-   n_jobs :적합성과 예측성을 위해 병렬로 실행할 작업 수\n",
    "\t-   random_state : 난수 seed 설정\n",
    "\t-   verbose : 실행 과정 출력 여부\n",
    "\t-   warm_start : 이전 호출의 솔루션을 재사용하여 합계에 더 많은 견적가를 추가\n",
    "\t-   class_weight : 클래스 가중치\n",
    "* 랜덤 포레스트도 feature selection 에 이용할 수 있어 !\n",
    "rf_clf1 =  RandomForestClassifier(n_estimators=300,  max_depth=10,  min_samples_leaf=8,  min_samples_split=8,  random_state=0)\n",
    "ftr_importances_value = rf_clf1.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_value,index=X_train.columns)\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "#### 부스팅\n",
    "* 여러개 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류 개선해나가면서 학습하는 방식\n",
    "\n",
    "#### Ada Boost  : AdaBoostClassifier()\n",
    "\n",
    "-   GradientBoosting이 이전 분류기가 만든 잔여 오차에 대해 새로운 분류기를 만들었다면, AdaBoost는 이전의 분류기가 잘못 분류한 샘플에 대해서 가중치를 높여서 다음 모델을 훈련시킴. 반복마다 샘플(오분류된 데이터)에 대한 가중치를 수정하는 것이다.\n",
    "-   훈련된 각 분류기는 성능에 따라 가중치가 부여되고, 예측에서 각 분류기가 예측한 레이블을 기준으로 가중치까지 적용해 가장 높은 값을 가진 레이블을 선택\n",
    "-   순차적 학습을 해야하는 만큼 병령수행은 불가능\n",
    "- parameter\n",
    "\t-  base_estimator : 앙상블에 포함될 기본 분류기종류, 기본값으로 DecisionTreeClassifier(max_depth=1)를 사용\n",
    "\t-   n_estimators : 모델에 포함될 분류기 개수\n",
    "\t-   learning_rate : 학습률\n",
    "\t-   algorithm : 부스팅 알고리즘, 기본값='SAMME.R'\n",
    "\n",
    "#### GBM sklearn.ensemble.GradientBoostingClassifier\n",
    "\n",
    "* class sklearn.ensemble.GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "-   여러 개의 결정 트리를 묶어 부스팅하는 앙상블 기법\n",
    "-   회귀와 분류 모두에 사용 가능\n",
    "-   랜덤하게 트리를 생성한 랜덤 포레스트와는 달리 그래디언트 부스팅은 이전 트리의 **오차를 보완**하는 방식으로 순차적으로 트리를 만듬 (무작위성이 없음)\n",
    "-   보통 다섯 개 이하 깊이의 트리를 사용하므로 메모리를 적게 사용하고 예측도 빠르다.\n",
    "-   각각의 트리는 데이터의 일부에 대해서만 맞춰져있어 트리가 많이 추가될수록 성능이 좋아짐\n",
    "-   단점 : 매개변수를 잘 조정해야하며, 훈련시간이 길다. 트리의 특성상 고차원 희소 데이터에 대해서는 정상적 작동이 어려움.\n",
    "* parameter\n",
    "\t- loss: 최적화 할 손실함수. 'deviance'는 이탈 여부의 확률적결과에 따른 분류(로지스틱), 'exponential'은 adaboost 알고리즘을 사용.\n",
    "\t- learning rate: 모델이 한 번 학습할 때 학습량(오차를 얼마나 보정할 것인가)을 지정. 기울기에 학습률을 곱해서 나온 값으로 가중치를 업데이트 함. 학습률이 너무 작으면 가중치 갱신이 거의 되지 않고 너무 크면 과적합 위험이 있음. 적절한 학습률 설정이 중요.\n",
    "\t- n_estimator: 작업을 수행할 boosting 단계를 지정.(tree의 개수지정). 많을수록 복잡한 모델\n",
    "\t- subsample: 개별 학습기들의 fitting에 사용할 샘플 비율. 1.0보다 작으면 Stochastic fradient boosting.\n",
    "\t- criterion: 분할의 품질을 측정하는 기능. (friedman_mse, squared_error, mae)\n",
    "\t- vaidation_fraction: 검증용 데이터 셋(0~1 값을 가지며 n_iter_no_change 값이 지정될 경우만 사용)\n",
    "\t- n_iter_no_change: 지정한 횟수 동안 반복하였는데 validation_fraction에 지정된 비율만큼 점수가 좋아지지 않으면 훈련을 조기 종료\n",
    "\n",
    "#### Xgboost\n",
    "* gradient boosting 알고리즘의 느리고 과적합 이슈가 있는 단점을 보완\n",
    "* gbm 보다 빠르고 자동 가지치기를 통해 과적합이 잘 안일어남\n",
    "* 다른 알고리즘과 연계 활용성이 좋음\n",
    "* 다양한 커스텀 최적화 옵션 제공 ex) 조기 중단 기능\n",
    "> xgboost.XGBRegressor(*, objective='reg:squarederror', **kwargs)\n",
    "\n",
    "* 상관관계 coef_ return 하는 attribute 제공 feature_importances_로 피처 셀렉션도 가능, 교차검증도 제공해줌!\n",
    "---\n",
    "### SVM sklearn.svm.SVC\n",
    ">class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "-  SVM은 분류에 사용되는 지도학습 머신러닝 모델\n",
    "- SVM은 결정 경계, 즉 분류를 위한 기준 선을 정의하는 모델  -> 분류되지 않은 새로운 점이 나타나면 경계의 어느 쪽에 속하는지 확인해서 분류 과제를 수행\n",
    "- 결정 경계는 데이터 군으로부터 최대한 멀리 떨어지는 게 좋아 (마진 최대화)\n",
    "- C와 gamma 통해 튜닝 / `C`, gammma 값이 클수록 하드마진(오류 허용 안 함), 작을수록 소프트마진(오류를 허용함)\n",
    "- parameter\n",
    "\t- C: 오류를 어느 정도까지 허용할 것인지를 지정. 값이 클수록 오류를 허용하지 않는 하드마진, 작을수록 오류를 허용하는 소프트마진\n",
    "\t- kernal: 알고리즘에 사용할 kernal 유형 지정(linear, poly, rbf, sigmoid, precomputed, default= 'rbf'). 결정 경계를 어떤 알고리즘을 사용?\n",
    "\t- degree: kernal 이 다항(poly)일 때만 사용. 몇 차항인지 지정\n",
    "\t- gamma: 커널의 계수값. rbf, poly, sigmoid일 때 사용. 결정경계를 얼마나 유연하게 그을 것인지를 지정. 값이 높을 경우 오버피팅의 위험, 낮을 경우 언더피팅의 위험\n",
    "\t- coef: 커널함수의 독립상수. poly, sigmoid 커널에서만 유효\n",
    "\t- shrinking: shirinking heurisitc을 사용할지 여부를 지정. default= True\n",
    "\t- probability: 확률 추정을 활성화할지 여부를 결정. default = False\n",
    "* 커널 : 머신러닝 모델이 약간의 오차를 허용해야 하는 건 너무나 당연한 거라 단순히 outlier 때문에 선형으로 분리할 수 ​​없다고 판단해서 안 돼. / 일부 아웃라이어에 맞추기 위해 비선형으로 결정 경계를 만들 필요가 없다 모든 점을 올바르게 분리하는 선을 그린다는 건 결국 모델이 데이터에 과도하게 적합해진다는 오버피팅 된다는 뜻\n",
    "\n",
    "---\n",
    "### KNN sklearn.neighbors.KNeighborsClassifier\n",
    "> _class_ sklearn.neighbors.KNeighborsClassifier(_n_neighbors=5_, _*_, _weights='uniform'_, _algorithm='auto'_, _leaf_size=30_, _p=2_, _metric='minkowski'_, _metric_params=None_, _n_jobs=None_)\n",
    "- 어떤 데이터가 주어지면 그 주변(이웃)의 데이터를 살펴본 뒤 더 많은 데이터가 포함되어 있는 범주로 분류하는 방식\n",
    "- parameter\n",
    "\t- n_neighbors: 탐색할 이웃 개수\n",
    "\t- weights: 예측에 사용하는 가중치 함수 (uniform, distance, callable(사용자지정)\n",
    "\t- algorithm: 가까운 이웃을 계산하기 위한 알고리즘 (auto, ball_tree, kd_tree, brute)\n",
    "\t- leaf_size:  전달되는 leaf 크기. 너무 작으면 가지 수가 너무 많아져서 속도가 느리고, 너무 크면 대충 분류되어 성능이 낮아질 수 있음\n",
    "\t- p: 민코프스키 거리값\n",
    "\t- metric: 거리 측정 방식을 변경하는 값. (euclidean, manhattan, chebyshev, mahalanobis)\n",
    "\t- metric_params: metric 함수의 추가 키워드\n",
    "\t- n_jobs: 이웃을 탐색하기 위해 사용하는 병렬 작업 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a256e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
