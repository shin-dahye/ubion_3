{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부스팅\n",
    "* 여러개 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류 개선해나가면서 학습하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ada Boost\n",
    "* 오분류된 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBM sklearn.ensemble.GradientBoostingClassifier\n",
    "* class sklearn.ensemble.GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 정확도 : 0.9389\n",
      "GBM 수행시간 : 557.4초\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from modules import hu\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = hu.get_human_dataset()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(f'GBM 정확도 : {gb_accuracy:.4f}')\n",
    "print(f'GBM 수행시간 : {time.time() - start_time:.1f}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = { 'n_estimators' : [100,500], 'learning_rate': [0.05,0.1]}\n",
    "grid_cv = GridSearchCV(gb_clf, param_grid=params, cv=2, verbose=1)\n",
    "grid_cv.fit(X_train,y_train)\n",
    "print(f'최적 하이퍼 파라미터 \\n {grid_cv.best_params_}')\n",
    "print(f'최고 예측 정확도 : {grid_cv.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost class xgboost.XGBRegressor(*, objective='reg:squarederror', **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "-   **n_estimators**  ([_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "    \n",
    "-   **max_depth**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – Maximum tree depth for base learners.\n",
    "    \n",
    "-   **learning_rate**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Boosting learning rate (xgb’s “eta”)\n",
    "    \n",
    "-   **verbosity**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "    \n",
    "-   **objective**  ([_Union_](https://docs.python.org/3.6/library/typing.html#typing.Union \"(in Python v3.6)\")_[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_,_ [_Callable_](https://docs.python.org/3.6/library/typing.html#typing.Callable \"(in Python v3.6)\")_[__[_[_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_,_ [_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_]__,_ [_Tuple_](https://docs.python.org/3.6/library/typing.html#typing.Tuple \"(in Python v3.6)\")_[_[_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_,_ [_numpy.ndarray_](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray \"(in NumPy v1.22)\")_]__]__,_ _NoneType__]_) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n",
    "    \n",
    "-   **booster**  (_Optional__[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_]_) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "    \n",
    "-   **tree_method**  (_Optional__[_[_str_](https://docs.python.org/3.6/library/stdtypes.html#str \"(in Python v3.6)\")_]_) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document:  [https://xgboost.readthedocs.io/en/latest/treemethod.html](https://xgboost.readthedocs.io/en/latest/treemethod.html).\n",
    "    \n",
    "-   **n_jobs**  (_Optional__[_[_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]_) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n",
    "    \n",
    "-   **gamma**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    \n",
    "-   **min_child_weight**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "    \n",
    "-   **max_delta_step**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Maximum delta step we allow each tree’s weight estimation to be.\n",
    "    \n",
    "-   **subsample**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of the training instance.\n",
    "    \n",
    "-   **colsample_bytree**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns when constructing each tree.\n",
    "    \n",
    "-   **colsample_bylevel**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns for each level.\n",
    "    \n",
    "-   **colsample_bynode**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Subsample ratio of columns for each split.\n",
    "    \n",
    "-   **reg_alpha**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – L1 regularization term on weights (xgb’s alpha).\n",
    "    \n",
    "-   **reg_lambda**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – L2 regularization term on weights (xgb’s lambda).\n",
    "    \n",
    "-   **scale_pos_weight**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – Balancing of positive and negative weights.\n",
    "    \n",
    "-   **base_score**  (_Optional__[_[_float_](https://docs.python.org/3.6/library/functions.html#float \"(in Python v3.6)\")_]_) – The initial prediction score of all instances, global bias.\n",
    "    \n",
    "-   **random_state**  (_Optional__[__Union__[_[_numpy.random.RandomState_](https://numpy.org/doc/stable/reference/random/legacy.html#numpy.random.RandomState \"(in NumPy v1.22)\")_,_ [_int_](https://docs.python.org/3.6/library/functions.html#int \"(in Python v3.6)\")_]__]_) –\n",
    "    \n",
    "    Random number seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
